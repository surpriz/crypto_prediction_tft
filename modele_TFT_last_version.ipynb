{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24526e4096e6417dbbf8e38f7831011f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8300aa916c7a48fbb9b64428df6eded0",
              "IPY_MODEL_c73a1b0039c540919bf397fa0014dda3",
              "IPY_MODEL_6071e18bbe934a189b1cdf8e80f854ec"
            ],
            "layout": "IPY_MODEL_a9e09000d1af48428cc189f537f53d62"
          }
        },
        "8300aa916c7a48fbb9b64428df6eded0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3b96a9b83454bf08800bbfac6d8ce59",
            "placeholder": "​",
            "style": "IPY_MODEL_d31a0f5af3f9420683e536366447ffef",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "c73a1b0039c540919bf397fa0014dda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b0807a06d534c79932c43f788d45a6f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2434980310124907a7141e9a2d7c5616",
            "value": 1
          }
        },
        "6071e18bbe934a189b1cdf8e80f854ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75a89e514aff44308a0cbc5cf6beee01",
            "placeholder": "​",
            "style": "IPY_MODEL_80bb307d55f948d888bc23015cc47bc3",
            "value": " 1/1 [00:00&lt;00:00,  3.80it/s]"
          }
        },
        "a9e09000d1af48428cc189f537f53d62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "f3b96a9b83454bf08800bbfac6d8ce59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d31a0f5af3f9420683e536366447ffef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b0807a06d534c79932c43f788d45a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2434980310124907a7141e9a2d7c5616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75a89e514aff44308a0cbc5cf6beee01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80bb307d55f948d888bc23015cc47bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2200c7532a054ac19d6b8b4e1d799eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d19815c88fb0469eb6a871c9b37ad3e8",
              "IPY_MODEL_fbe0ceaaaead414b87819c423333ad1b",
              "IPY_MODEL_edcb72d0812043d08503b2bb2d384260"
            ],
            "layout": "IPY_MODEL_8d009ddabff9490fb6ca61e94bd6c857"
          }
        },
        "d19815c88fb0469eb6a871c9b37ad3e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7d22425dbf04334810cae6923e033ac",
            "placeholder": "​",
            "style": "IPY_MODEL_5437fad04e8f432cbad851dd4bbe6f32",
            "value": "Epoch 0:   0%"
          }
        },
        "fbe0ceaaaead414b87819c423333ad1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92bad7796ec04cea87e6907fad4be3e4",
            "max": 97,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b228479e6977438c8f25bc3a631910eb",
            "value": 0
          }
        },
        "edcb72d0812043d08503b2bb2d384260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_660be3b0a0ef44ac8c7b2204db8cf49b",
            "placeholder": "​",
            "style": "IPY_MODEL_52e5fb9a274c49a687ce1ec5d7e7af96",
            "value": " 0/97 [00:00&lt;?, ?it/s]"
          }
        },
        "8d009ddabff9490fb6ca61e94bd6c857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "f7d22425dbf04334810cae6923e033ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5437fad04e8f432cbad851dd4bbe6f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92bad7796ec04cea87e6907fad4be3e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b228479e6977438c8f25bc3a631910eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "660be3b0a0ef44ac8c7b2204db8cf49b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52e5fb9a274c49a687ce1ec5d7e7af96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwzal6R-My8Y",
        "outputId": "bca8585a-49d7-43fd-f6d0-3e20ffb7ac85"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.5.0.post0)\n",
            "Requirement already satisfied: pytorch-forecasting in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: ta in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.19.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.2.51)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->-r requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning->-r requirements.txt (line 2)) (0.11.9)\n",
            "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting->-r requirements.txt (line 3)) (2.5.0.post0)\n",
            "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (75.1.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 6)) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 6)) (5.3.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 6)) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 6)) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 6)) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance->-r requirements.txt (line 6)) (1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.2.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirements.txt (line 6)) (2.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (3.11.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance->-r requirements.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 5)) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import pytorch_forecasting\n",
        "    print(\"PyTorch Forecasting est correctement installé !\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"Erreur : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcJIT0hYM3nu",
        "outputId": "0eefaf00-0728-49bd-b31e-620cf8fccb13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Forecasting est correctement installé !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivDZURQYAs_U",
        "outputId": "179e1427-7910-4726-e81e-b8bc35e4d03e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Importations et Pré-requis\n",
        "# -----------------------------------------------\n",
        "\n",
        "# 1. Bibliothèques Standard\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "\n",
        "# 2. Bibliothèques Tierces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import yfinance as yf\n",
        "import wandb\n",
        "import ta  # Bibliothèque pour les indicateurs techniques\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import QuantileLoss\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('log_importation_library.log'),  # Sauvegarde dans un fichier\n",
        "        logging.StreamHandler()              # Affichage dans Colab\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "pBsUEF-TMjMz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Configuration WandB pour TFT\n",
        "# -----------------------------------------------\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('log_configuration_WandB.log'),  # Sauvegarde dans un fichier\n",
        "        logging.StreamHandler()              # Affichage dans Colab\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "def init_wandb_config(stock_symbol, mode):\n",
        "    \"\"\"\n",
        "    Initialise la configuration WandB complète pour le modèle TFT.\n",
        "\n",
        "    Args:\n",
        "        stock_symbol (str): Symbole du stock à analyser (par défaut 'BTC-USD').\n",
        "        mode (str): Mode de WandB ('disabled' pour désactiver les logs).\n",
        "\n",
        "    Returns:\n",
        "        dict: Configuration WandB initialisée.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Initialisation de la configuration WandB...\")\n",
        "\n",
        "        # Forcer la fermeture de toute session WandB existante\n",
        "        wandb.finish()\n",
        "\n",
        "        # Initialisation de WandB avec la configuration complète\n",
        "        run = wandb.init(\n",
        "            project=\"stock_predictions_TFT\",\n",
        "            name=f\"tft_{stock_symbol}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
        "            tags=[\n",
        "                stock_symbol,\n",
        "                \"multi_horizon\",\n",
        "                \"Deep Learning\",\n",
        "                \"TFT\",\n",
        "                \"Complete_Pipeline\",\n",
        "            ],\n",
        "            mode=mode,\n",
        "            config={\n",
        "                \"data\": {\n",
        "                    \"symbol\": stock_symbol,\n",
        "                    \"end_date\": datetime.now().strftime('%Y-%m-%d'),\n",
        "                },\n",
        "                \"preprocessing\": {\n",
        "                    \"sort_index\": True,\n",
        "                    \"handle_missing\": {\n",
        "                        \"method\": \"drop\",  # Options: ['drop', 'fill_forward', 'interpolate']\n",
        "                        \"max_missing_ratio\": 0.1,\n",
        "                        \"interpolation_method\": \"linear\",\n",
        "                    },\n",
        "                    \"normalization\": {\n",
        "                        \"type\": \"standard\",  # ['standard', 'robust', 'minmax']\n",
        "                        \"center\": True,\n",
        "                        \"exclude_features\": [\"Year\", \"Month\", \"Day\", \"series_id\"],  # Retiré \"Day_of_Week\"\n",
        "                        \"clip\": {  # Clipping des valeurs\n",
        "                            \"enabled\": False,\n",
        "                            \"threshold\": 3,  # Nombre d'écarts-types\n",
        "                        },\n",
        "                        \"normalize_target\": False,  # Ne pas normaliser la cible\n",
        "                    },\n",
        "                    \"required_columns\": [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n",
        "\n",
        "                    \"target_transform\": {\n",
        "                        \"method\": \"returns\",\n",
        "                        \"prediction_mode\": \"cumulative\",\n",
        "                        \"periods\": list(range(1, 31)),\n",
        "                        \"use_log_returns\": True,\n",
        "                        \"scaling\": {\n",
        "                            \"type\": \"standard\",  # Standardisation des returns\n",
        "                            \"center\": True,\n",
        "                            \"clip_threshold\": 3.0  # Clip les returns extrêmes à 3 écarts-types\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                \"technical_indicators\": {\n",
        "                    \"moving_averages\": {\n",
        "                        \"sma_periods\": [10, 30],\n",
        "                        \"ema_periods\": [10],\n",
        "                        \"volume_ma_period\": 10,\n",
        "                    },\n",
        "                    \"oscillators\": {\n",
        "                        \"rsi\": {\n",
        "                            \"period\": 14,\n",
        "                            \"overbought\": 70,\n",
        "                            \"oversold\": 30,\n",
        "                        },\n",
        "                        \"stochastic\": {\n",
        "                            \"k_period\": 14,\n",
        "                            \"d_period\": 3,\n",
        "                            \"overbought\": 80,\n",
        "                            \"oversold\": 20,\n",
        "                        },\n",
        "                    },\n",
        "                    \"bollinger_bands\": {\n",
        "                        \"period\": 20,\n",
        "                        \"std_dev\": 2,\n",
        "                    },\n",
        "                    \"macd\": {\n",
        "                        \"fast_period\": 12,\n",
        "                        \"slow_period\": 26,\n",
        "                        \"signal_period\": 9,\n",
        "                    },\n",
        "                    \"volume_analysis\": {\n",
        "                        \"enable_volume_ma\": True,\n",
        "                        \"enable_volume_change\": True,\n",
        "                        \"volume_ma_period\": 10,  # Ajouté pour correspondre à normalize_data\n",
        "                    },\n",
        "                    \"price_momentum\": {\n",
        "                        \"enable_roc\": True,\n",
        "                        \"roc_period\": 1,\n",
        "                    },\n",
        "                    \"atr\": {\n",
        "                        \"window\": 14,\n",
        "                    },\n",
        "                    \"adx\": {\n",
        "                        \"window\": 14,\n",
        "                    },\n",
        "                    \"ichimoku_cloud\": {\n",
        "                        \"enabled\": True,  # Ajouté pour activer Ichimoku\n",
        "                        \"window1\": 9,\n",
        "                        \"window2\": 26,\n",
        "                        \"window3\": 52\n",
        "                    },\n",
        "                    \"vwap\": {\n",
        "                        \"window\": 14,\n",
        "                    },\n",
        "                    \"cmf\": {\n",
        "                        \"window\": 20,\n",
        "                    },\n",
        "                    \"cci\": {\n",
        "                        \"window\": 20,\n",
        "                    },\n",
        "                    \"trix\": {\n",
        "                        \"window\": 15,\n",
        "                    },\n",
        "                    \"williams_r\": {\n",
        "                        \"window\": 14,\n",
        "                    },\n",
        "                },\n",
        "                \"feature_engineering\": {\n",
        "                    \"price_features\": {\n",
        "                        \"use_open\": True,\n",
        "                        \"use_high\": True,\n",
        "                        \"use_low\": True,\n",
        "                        \"use_close\": True,\n",
        "                        \"use_volume\": True,\n",
        "                    },\n",
        "                    \"advanced_features\": {\n",
        "                        \"use_market_features\": True,  # Features universelles\n",
        "                        \"use_extended_features\": True,  # Features étendues\n",
        "                        \"market_windows\": [5, 10, 20],  # Fenêtres pour les calculs\n",
        "                        \"volatility_windows\": [5, 10, 20],  # Fenêtres pour la volatilité\n",
        "                        \"volume_analysis\": {\n",
        "                            \"enable_volume_patterns\": True,\n",
        "                            \"enable_volume_profile\": True,\n",
        "                            \"profile_window\": 10\n",
        "                        },\n",
        "                        \"momentum_analysis\": {\n",
        "                            \"enable_acceleration\": True,\n",
        "                            \"enable_efficiency\": True,\n",
        "                            \"efficiency_windows\": [5, 10, 20]\n",
        "                        },\n",
        "                        \"pattern_recognition\": {\n",
        "                            \"enable_candlestick_patterns\": True,\n",
        "                            \"enable_support_resistance\": True,\n",
        "                            \"sr_windows\": [10, 20, 50]\n",
        "                        },\n",
        "                    },  # Fermeture de 'advanced_features'\n",
        "                    \"technical_features\": {\n",
        "                        \"moving_averages\": [\"SMA_10\", \"SMA_30\", \"EMA_10\"],\n",
        "                        \"momentum\": [\"ROC\", \"ATR\", \"ADX\"],\n",
        "                        \"oscillators\": [\"RSI\", \"Stochastic_%K\", \"Stochastic_%D\", \"MACD\", \"MACD_Signal\", \"MACD_Diff\"],\n",
        "                        \"bollinger_bands\": [\"Bollinger_Middle\", \"Bollinger_Upper\", \"Bollinger_Lower\"],\n",
        "                        \"ichimoku_cloud\": [\"Ichimoku_Conversion_Line\", \"Ichimoku_Base_Line\", \"Ichimoku_A\", \"Ichimoku_B\"],\n",
        "                        \"volume_analysis\": [\"Volume_MA_10\", \"Volume_Change\"],\n",
        "                        \"vwap\": [\"VWAP\"],\n",
        "                        \"cmf\": [\"CMF\"],\n",
        "                        \"cci\": [\"CCI\"],\n",
        "                        \"trix\": [\"TRIX\"],\n",
        "                        \"williams_r\": [\"Williams_%R\"],\n",
        "                        \"time_varying_known_categoricals\": [\"Day_of_Week\"],  # Ajouté ici\n",
        "                    },\n",
        "                    \"temporal_features\": {\n",
        "                        \"use_year\": True,\n",
        "                        \"use_month\": True,\n",
        "                        \"use_day\": True,\n",
        "                        \"use_day_of_week\": True,  # Utilisation de \"Day_of_Week\"\n",
        "                    },\n",
        "                    \"target\": \"Close\",\n",
        "                },\n",
        "                \"data_split\": {\n",
        "                    \"train_ratio\": 0.8,\n",
        "                    \"validation_ratio\": 0.1,\n",
        "                    \"test_ratio\": 0.1,\n",
        "                    \"shuffle\": False,  # Important pour les séries temporelles\n",
        "                },\n",
        "                \"pytorch_forecasting\": {\n",
        "                    \"time_series_params\": {\n",
        "                        \"max_prediction_length\": 30,\n",
        "                        \"max_encoder_length\": 180,\n",
        "                        \"min_encoder_length\": 30,\n",
        "                        \"min_prediction_length\": 1,\n",
        "                    },\n",
        "                    \"target_params\": {\n",
        "                        \"target\": \"Close\",\n",
        "                        \"normalizer\": {\n",
        "                            \"type\": \"group\",\n",
        "                            \"transformation\": \"softplus\",\n",
        "                            \"center\": True,\n",
        "                        },\n",
        "                    },\n",
        "                    \"training_params\": {\n",
        "                        \"batch_size\": {\"value\": 32},\n",
        "                        \"num_workers\": 0,\n",
        "                        \"shuffle_training\": True,\n",
        "                    },\n",
        "                    \"dataset_params\": {\n",
        "                        \"add_relative_time_idx\": True,\n",
        "                        \"add_target_scales\": True,\n",
        "                        \"add_encoder_length\": True,\n",
        "                    },\n",
        "                },\n",
        "                \"model\": {\n",
        "                    \"architecture\": {\n",
        "                        \"hidden_size\": {\"value\": 64},\n",
        "                        \"attention_head_size\": {\"value\": 4},\n",
        "                        \"dropout\": {\"value\": 0.1},\n",
        "                        \"hidden_continuous_size\": {\"value\": 32},\n",
        "                        \"loss_fn\": \"QuantileLoss\",\n",
        "                        \"log_interval\": 10,\n",
        "                    },\n",
        "                    \"training\": {\n",
        "                        \"learning_rate\": {\"value\": 0.001},\n",
        "                        \"optimizer\": \"adam\",\n",
        "                        \"max_epochs\": {\"value\": 30},\n",
        "                        \"gradient_clip_val\": 0.1,\n",
        "                        \"reduce_on_plateau_patience\": 4,\n",
        "                    },\n",
        "                    \"early_stopping\": {\n",
        "                        \"monitor\": \"val_loss\",\n",
        "                        \"min_delta\": 1e-4,\n",
        "                        \"patience\": 10,\n",
        "                        \"mode\": \"min\",\n",
        "                    },\n",
        "                    \"scheduler\": {\n",
        "                        \"factor\": 0.1,\n",
        "                        \"patience\": 4,\n",
        "                        \"mode\": \"min\",\n",
        "                    },\n",
        "                },\n",
        "                \"evaluation\": {\n",
        "                    \"metrics\": {\n",
        "                        \"mae\": True,\n",
        "                        \"mse\": True,\n",
        "                        \"rmse\": True,\n",
        "                        \"r2\": True,\n",
        "                        \"mape\": True,\n",
        "                        \"mase\": True,\n",
        "                        \"directional_accuracy\": True,\n",
        "                        \"rolling_window\": 30,\n",
        "                    },\n",
        "                    \"prediction\": {\n",
        "                        \"batch_size\": 1,\n",
        "                        \"num_workers\": 0,\n",
        "                        \"quantile\": {  # Intervalles de confiance\n",
        "                            \"lower\": 0.1,\n",
        "                            \"upper\": 0.9\n",
        "                        },\n",
        "                    },\n",
        "                    \"visualization\": {\n",
        "                        \"plot_predictions\": True,\n",
        "                        \"plot_errors\": True,\n",
        "                        \"figure_size\": {\"width\": 15, \"height\": 12},\n",
        "                        \"max_error_annotations\": 3,\n",
        "                        \"show_metrics_on_plot\": True,\n",
        "                        \"metrics_box_position\": {\n",
        "                            \"x\": 0.02,\n",
        "                            \"y\": 0.98,\n",
        "                            \"font_size\": 10\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\"Configuration WandB initialisée avec succès.\")\n",
        "        return run.config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'initialisation de la configuration WandB: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        raise e\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Code d'exécution principal\n",
        "# -----------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # On définit les valeurs une seule fois ici\n",
        "        stock_symbol = 'BTC-USD'\n",
        "        mode = 'disabled'\n",
        "\n",
        "        print(\"\\n=== DÉMARRAGE DE L'INITIALISATION WANDB ===\")\n",
        "        config = init_wandb_config(stock_symbol, mode)\n",
        "\n",
        "        # Vérification de la configuration\n",
        "        if config:\n",
        "            print(\"\\n=== VÉRIFICATION DE LA CONFIGURATION ===\")\n",
        "            print(f\"Symbole configuré: {config['data']['symbol']}\")\n",
        "            print(f\"Date de fin: {config['data']['end_date']}\")\n",
        "            print(\"\\nConfiguration des indicateurs techniques:\")\n",
        "            print(f\"- Périodes SMA: {config['technical_indicators']['moving_averages']['sma_periods']}\")\n",
        "            print(f\"- Période RSI: {config['technical_indicators']['oscillators']['rsi']['period']}\")\n",
        "\n",
        "            print(\"\\nConfiguration de la normalisation:\")\n",
        "            print(f\"- Type: {config['preprocessing']['normalization']['type']}\")\n",
        "            print(f\"- Normalisation de la cible: {config['preprocessing']['normalization']['normalize_target']}\")\n",
        "\n",
        "            print(\"\\n=== CONFIGURATION WANDB RÉUSSIE ===\")\n",
        "        else:\n",
        "            print(\"\\n!!! ERREUR: La configuration n'a pas été initialisée correctement !!!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n!!! ERREUR LORS DE L'INITIALISATION !!!\")\n",
        "        print(f\"Nature de l'erreur: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # S'assurer que wandb est bien fermé en cas d'erreur\n",
        "        try:\n",
        "            wandb.finish()\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "Qi5kzqW6CJZg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "83d7fd27-75e6-43d7-f8ab-fd4da219d2f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DÉMARRAGE DE L'INITIALISATION WANDB ===\n",
            "Initialisation de la configuration WandB...\n",
            "Configuration WandB initialisée avec succès.\n",
            "\n",
            "=== VÉRIFICATION DE LA CONFIGURATION ===\n",
            "Symbole configuré: BTC-USD\n",
            "Date de fin: 2025-01-18\n",
            "\n",
            "Configuration des indicateurs techniques:\n",
            "- Périodes SMA: [10, 30]\n",
            "- Période RSI: 14\n",
            "\n",
            "Configuration de la normalisation:\n",
            "- Type: standard\n",
            "- Normalisation de la cible: False\n",
            "\n",
            "=== CONFIGURATION WANDB RÉUSSIE ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Téléchargement et Préparation des Données\n",
        "# -----------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "import logging\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('log_telechargement_data.log'),  # Sauvegarde dans un fichier\n",
        "        logging.StreamHandler()              # Affichage dans Colab\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ajoute les caractéristiques temporelles au DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame avec un index datetime\n",
        "\n",
        "    Returns:\n",
        "        DataFrame enrichi avec les caractéristiques temporelles\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Extraction des caractéristiques temporelles de l'index\n",
        "    df['Year'] = df.index.year\n",
        "    df['Month'] = df.index.month\n",
        "    df['Day'] = df.index.day\n",
        "    df['Day_of_Week'] = df.index.dayofweek  # 0 = Lundi, 6 = Dimanche\n",
        "\n",
        "    return df\n",
        "\n",
        "def download_and_prepare_data(config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Télécharge et prépare les données financières depuis Yahoo Finance.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration contenant 'data' et 'preprocessing'.\n",
        "            data: Doit contenir 'symbol' et optionnellement 'end_date'.\n",
        "            preprocessing: Doit contenir 'required_columns' et 'handle_missing'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Données financières préparées.\n",
        "    \"\"\"\n",
        "    # Validation de base\n",
        "    symbol = config.get('data', {}).get('symbol')\n",
        "    if not symbol:\n",
        "        raise ValueError(\"Le symbole du stock est requis\")\n",
        "\n",
        "    end_date = config.get('data', {}).get('end_date', datetime.now().strftime('%Y-%m-%d'))\n",
        "    required_cols = set(config.get('preprocessing', {}).get('required_columns', []))\n",
        "    missing_method = config.get('preprocessing', {}).get('handle_missing', {}).get('method', 'drop')\n",
        "\n",
        "    # Téléchargement des données\n",
        "    logger.info(f\"Téléchargement des données pour {symbol}\")\n",
        "    data = yf.download(symbol, end=end_date)\n",
        "\n",
        "    if data.empty:\n",
        "        raise ValueError(f\"Aucune donnée disponible pour {symbol}\")\n",
        "\n",
        "    # Gestion du MultiIndex des colonnes\n",
        "    if isinstance(data.columns, pd.MultiIndex):\n",
        "        data.columns = [col[0] for col in data.columns]\n",
        "\n",
        "    # Vérification des colonnes requises\n",
        "    if required_cols:\n",
        "        available_cols = set(data.columns)\n",
        "        missing = required_cols - available_cols\n",
        "        if missing:\n",
        "            logger.error(f\"Colonnes disponibles: {available_cols}\")\n",
        "            raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
        "\n",
        "    # Ajout des caractéristiques temporelles\n",
        "    data = add_temporal_features(data)\n",
        "\n",
        "    # Gestion des valeurs manquantes\n",
        "    if missing_method == 'drop':\n",
        "        data = data.dropna()\n",
        "    elif missing_method == 'fill_forward':\n",
        "        data = data.ffill()\n",
        "    elif missing_method == 'interpolate':\n",
        "        data = data.interpolate(method='linear')\n",
        "\n",
        "    logger.info(f\"Données préparées: {len(data)} entrées\")\n",
        "    return data\n",
        "\n",
        "# Appel de la fonction\n",
        "df = download_and_prepare_data(config)\n",
        "\n",
        "# Afficher les premières lignes pour vérifier\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIYt9PLGHe5o",
        "outputId": "280ea8da-c5db-450d-9a2e-f8c7dd38f9e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Close        High         Low        Open    Volume  Year  \\\n",
            "Date                                                                         \n",
            "2014-09-17  457.334015  468.174011  452.421997  465.864014  21056800  2014   \n",
            "2014-09-18  424.440002  456.859985  413.104004  456.859985  34483200  2014   \n",
            "2014-09-19  394.795990  427.834991  384.532013  424.102997  37919700  2014   \n",
            "2014-09-20  408.903992  423.295990  389.882996  394.673004  36863600  2014   \n",
            "2014-09-21  398.821014  412.425995  393.181000  408.084991  26580100  2014   \n",
            "\n",
            "            Month  Day  Day_of_Week  \n",
            "Date                                 \n",
            "2014-09-17      9   17            2  \n",
            "2014-09-18      9   18            3  \n",
            "2014-09-19      9   19            4  \n",
            "2014-09-20      9   20            5  \n",
            "2014-09-21      9   21            6  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Ajout des Indicateurs Techniques\n",
        "# -----------------------------------------------\n",
        "\n",
        "import ta\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def add_technical_indicators(data: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ajoute les indicateurs techniques aux données financières.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame avec colonnes OHLCV\n",
        "        config: Dict de configuration des indicateurs\n",
        "\n",
        "    Returns:\n",
        "        DataFrame enrichi avec les indicateurs techniques\n",
        "    \"\"\"\n",
        "    df = data.copy()\n",
        "    tech_indicators = config.get('technical_indicators', {})\n",
        "\n",
        "    try:\n",
        "        # 1. Moyennes Mobiles\n",
        "        ma_config = tech_indicators.get('moving_averages', {})\n",
        "        for period in ma_config.get('sma_periods', []):\n",
        "            df[f'SMA_{period}'] = ta.trend.SMAIndicator(close=df['Close'], window=period).sma_indicator()\n",
        "            logger.info(f\"SMA_{period} calculé.\")\n",
        "        for period in ma_config.get('ema_periods', []):\n",
        "            df[f'EMA_{period}'] = ta.trend.EMAIndicator(close=df['Close'], window=period).ema_indicator()\n",
        "            logger.info(f\"EMA_{period} calculé.\")\n",
        "\n",
        "        # 2. Oscillateurs\n",
        "        osc_config = tech_indicators.get('oscillators', {})\n",
        "\n",
        "        # RSI\n",
        "        rsi_config = osc_config.get('rsi', {})\n",
        "        if rsi_config:\n",
        "            rsi_period = rsi_config.get('period', 14)\n",
        "            df['RSI'] = ta.momentum.RSIIndicator(close=df['Close'], window=rsi_period).rsi()\n",
        "            logger.info(f\"RSI (période {rsi_period}) calculé.\")\n",
        "\n",
        "        # Stochastique\n",
        "        stoch_config = osc_config.get('stochastic', {})\n",
        "        if stoch_config:\n",
        "            stoch = ta.momentum.StochasticOscillator(\n",
        "                high=df['High'], low=df['Low'], close=df['Close'],\n",
        "                window=stoch_config.get('k_period', 14),\n",
        "                smooth_window=stoch_config.get('d_period', 3)\n",
        "            )\n",
        "            df['Stochastic_%K'] = stoch.stoch()\n",
        "            df['Stochastic_%D'] = stoch.stoch_signal()\n",
        "            logger.info(\"Stochastic_%K et Stochastic_%D calculés.\")\n",
        "\n",
        "        # 3. Bandes de Bollinger\n",
        "        bb_config = tech_indicators.get('bollinger_bands', {})\n",
        "        if bb_config:\n",
        "            bollinger = ta.volatility.BollingerBands(\n",
        "                close=df['Close'],\n",
        "                window=bb_config.get('period', 20),\n",
        "                window_dev=bb_config.get('std_dev', 2)\n",
        "            )\n",
        "            df[['Bollinger_Middle', 'Bollinger_Upper', 'Bollinger_Lower']] = pd.concat([\n",
        "                bollinger.bollinger_mavg(),\n",
        "                bollinger.bollinger_hband(),\n",
        "                bollinger.bollinger_lband()\n",
        "            ], axis=1)\n",
        "            logger.info(\"Bandes de Bollinger calculées.\")\n",
        "\n",
        "        # 4. MACD\n",
        "        macd_config = tech_indicators.get('macd', {})\n",
        "        if macd_config:\n",
        "            macd = ta.trend.MACD(\n",
        "                close=df['Close'],\n",
        "                window_slow=macd_config.get('slow_period', 26),\n",
        "                window_fast=macd_config.get('fast_period', 12),\n",
        "                window_sign=macd_config.get('signal_period', 9)\n",
        "            )\n",
        "            df[['MACD', 'MACD_Signal', 'MACD_Diff']] = pd.concat([\n",
        "                macd.macd(), macd.macd_signal(), macd.macd_diff()\n",
        "            ], axis=1)\n",
        "            logger.info(\"MACD calculé.\")\n",
        "\n",
        "        # 5. Volume et Momentum\n",
        "        volume_analysis = tech_indicators.get('volume_analysis', {})\n",
        "        if volume_analysis.get('enable_volume_ma', False):\n",
        "            period = volume_analysis.get('volume_ma_period', 10)\n",
        "            df[f'Volume_MA_{period}'] = ta.trend.SMAIndicator(close=df['Volume'], window=period).sma_indicator()\n",
        "            logger.info(f\"Volume_MA_{period} calculé.\")\n",
        "\n",
        "        if volume_analysis.get('enable_volume_change', False):\n",
        "            df['Volume_Change'] = df['Volume'].pct_change() * 100\n",
        "            logger.info(\"Volume_Change calculé.\")\n",
        "\n",
        "        # 6. Indicateurs Supplémentaires\n",
        "        price_momentum = tech_indicators.get('price_momentum', {})\n",
        "        if price_momentum.get('enable_roc', False):\n",
        "            roc_period = price_momentum.get('roc_period', 1)\n",
        "            df['ROC'] = ta.momentum.ROCIndicator(close=df['Close'], window=roc_period).roc()\n",
        "            logger.info(f\"ROC (période {roc_period}) calculé.\")\n",
        "\n",
        "        # ATR\n",
        "        atr_config = tech_indicators.get('atr', {})\n",
        "        if atr_config:\n",
        "            atr_period = atr_config.get('window', 14)\n",
        "            df['ATR'] = ta.volatility.AverageTrueRange(\n",
        "                high=df['High'], low=df['Low'], close=df['Close'], window=atr_period\n",
        "            ).average_true_range()\n",
        "            logger.info(f\"ATR (période {atr_period}) calculé.\")\n",
        "\n",
        "        # ADX\n",
        "        adx_config = tech_indicators.get('adx', {})\n",
        "        if adx_config:\n",
        "            adx_period = adx_config.get('window', 14)\n",
        "            df['ADX'] = ta.trend.ADXIndicator(\n",
        "                high=df['High'], low=df['Low'], close=df['Close'], window=adx_period\n",
        "            ).adx()\n",
        "            logger.info(f\"ADX (période {adx_period}) calculé.\")\n",
        "\n",
        "        # Ichimoku Cloud\n",
        "        ichimoku_config = tech_indicators.get('ichimoku_cloud', {})\n",
        "        if ichimoku_config.get('enabled', False):\n",
        "            ichimoku = ta.trend.IchimokuIndicator(\n",
        "                high=df['High'], low=df['Low'],\n",
        "                window1=ichimoku_config.get('window1', 9),\n",
        "                window2=ichimoku_config.get('window2', 26),\n",
        "                window3=ichimoku_config.get('window3', 52)\n",
        "            )\n",
        "            df['Ichimoku_Conversion_Line'] = ichimoku.ichimoku_conversion_line()\n",
        "            df['Ichimoku_Base_Line'] = ichimoku.ichimoku_base_line()\n",
        "            df['Ichimoku_A'] = ichimoku.ichimoku_a()\n",
        "            df['Ichimoku_B'] = ichimoku.ichimoku_b()\n",
        "            logger.info(\"Indicateurs Ichimoku Cloud calculés.\")\n",
        "\n",
        "        # VWAP\n",
        "        vwap_config = tech_indicators.get('vwap', {})\n",
        "        if vwap_config:\n",
        "            vwap_period = vwap_config.get('window', 14)\n",
        "            df['VWAP'] = ta.volume.VolumeWeightedAveragePrice(\n",
        "                high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume'], window=vwap_period\n",
        "            ).volume_weighted_average_price()\n",
        "            logger.info(f\"VWAP (période {vwap_period}) calculé.\")\n",
        "\n",
        "        # CMF\n",
        "        cmf_config = tech_indicators.get('cmf', {})\n",
        "        if cmf_config:\n",
        "            cmf_period = cmf_config.get('window', 20)\n",
        "            df['CMF'] = ta.volume.ChaikinMoneyFlowIndicator(\n",
        "                high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume'], window=cmf_period\n",
        "            ).chaikin_money_flow()\n",
        "            logger.info(f\"CMF (période {cmf_period}) calculé.\")\n",
        "\n",
        "        # CCI\n",
        "        cci_config = tech_indicators.get('cci', {})\n",
        "        if cci_config:\n",
        "            cci_period = cci_config.get('window', 20)\n",
        "            df['CCI'] = ta.trend.CCIIndicator(\n",
        "                high=df['High'], low=df['Low'], close=df['Close'], window=cci_period\n",
        "            ).cci()\n",
        "            logger.info(f\"CCI (période {cci_period}) calculé.\")\n",
        "\n",
        "        # TRIX\n",
        "        trix_config = tech_indicators.get('trix', {})\n",
        "        if trix_config:\n",
        "            trix_period = trix_config.get('window', 15)\n",
        "            df['TRIX'] = ta.trend.TRIXIndicator(close=df['Close'], window=trix_period).trix()\n",
        "            logger.info(f\"TRIX (période {trix_period}) calculé.\")\n",
        "\n",
        "\n",
        "        # Williams %R - Correction du paramètre window en lbp\n",
        "        williams_r_config = tech_indicators.get('williams_r', {})\n",
        "        if williams_r_config:\n",
        "            williams_period = williams_r_config.get('window', 14)\n",
        "            df['Williams_%R'] = ta.momentum.WilliamsRIndicator(\n",
        "                high=df['High'],\n",
        "                low=df['Low'],\n",
        "                close=df['Close'],\n",
        "                lbp=williams_period  # Utilisation de lbp au lieu de window\n",
        "            ).williams_r()\n",
        "            logger.info(f\"Williams_%R (période {williams_period}) calculé.\")\n",
        "\n",
        "        # Nettoyage final\n",
        "        rows_before = len(df)\n",
        "        df.dropna(inplace=True)\n",
        "        logger.info(f\"Lignes: {rows_before} → {len(df)} après nettoyage\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur lors du calcul des indicateurs: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# 1. D'abord télécharger les données\n",
        "#df = download_and_prepare_data(config)\n",
        "\n",
        "# 2. Ensuite ajouter les indicateurs techniques\n",
        "df_with_indicators = add_technical_indicators(df, config)\n",
        "\n",
        "# Afficher les colonnes disponibles\n",
        "print(\"Colonnes disponibles :\")\n",
        "print(df_with_indicators.columns.tolist())\n",
        "\n",
        "# Afficher les premières lignes\n",
        "print(\"\\nPremières lignes :\")\n",
        "print(df_with_indicators.head())"
      ],
      "metadata": {
        "id": "Uq-fL2huHrU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8415fe-3736-481d-9496-1d653c369ff8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colonnes disponibles :\n",
            "['Close', 'High', 'Low', 'Open', 'Volume', 'Year', 'Month', 'Day', 'Day_of_Week', 'SMA_10', 'SMA_30', 'EMA_10', 'RSI', 'Stochastic_%K', 'Stochastic_%D', 'Bollinger_Middle', 'Bollinger_Upper', 'Bollinger_Lower', 'MACD', 'MACD_Signal', 'MACD_Diff', 'Volume_MA_10', 'Volume_Change', 'ROC', 'ATR', 'ADX', 'Ichimoku_Conversion_Line', 'Ichimoku_Base_Line', 'Ichimoku_A', 'Ichimoku_B', 'VWAP', 'CMF', 'CCI', 'TRIX', 'Williams_%R']\n",
            "\n",
            "Premières lignes :\n",
            "                 Close        High         Low        Open    Volume  Year  \\\n",
            "Date                                                                         \n",
            "2014-10-30  345.304993  350.912994  335.071991  335.709015  30177900  2014   \n",
            "2014-10-31  338.321014  348.045013  337.141998  345.009003  12545400  2014   \n",
            "2014-11-01  325.748993  340.528992  321.054993  338.649994  16677200  2014   \n",
            "2014-11-02  325.891998  329.049988  320.626007  326.075012   8603620  2014   \n",
            "2014-11-03  327.553986  334.002014  325.480988  325.569000  12948500  2014   \n",
            "\n",
            "            Month  Day  Day_of_Week      SMA_10  ...        ADX  \\\n",
            "Date                                             ...              \n",
            "2014-10-30     10   30            3  357.987302  ...  29.385268   \n",
            "2014-10-31     10   31            4  353.171902  ...  30.155534   \n",
            "2014-11-01     11    1            5  347.431003  ...  31.482423   \n",
            "2014-11-02     11    2            6  344.178503  ...  32.729523   \n",
            "2014-11-03     11    3            0  341.099402  ...  33.372402   \n",
            "\n",
            "            Ichimoku_Conversion_Line  Ichimoku_Base_Line  Ichimoku_A  \\\n",
            "Date                                                                   \n",
            "2014-10-30                361.823990          350.496994  356.160492   \n",
            "2014-10-31                360.059998          357.128998  358.594498   \n",
            "2014-11-01                342.699997          366.089996  354.394997   \n",
            "2014-11-02                340.305008          366.162003  353.233505   \n",
            "2014-11-03                340.305008          366.162003  353.233505   \n",
            "\n",
            "            Ichimoku_B        VWAP       CMF         CCI      TRIX  \\\n",
            "Date                                                                 \n",
            "2014-10-30  378.735001  363.901238 -0.034138 -116.414704 -0.172053   \n",
            "2014-10-31  378.735001  361.433858 -0.065463 -115.220598 -0.208529   \n",
            "2014-11-01  378.735001  357.543819 -0.132971 -137.162464 -0.250247   \n",
            "2014-11-02  378.735001  355.413905 -0.190408 -132.545165 -0.293295   \n",
            "2014-11-03  378.735001  351.701320 -0.219091 -109.869623 -0.334126   \n",
            "\n",
            "            Williams_%R  \n",
            "Date                     \n",
            "2014-10-30   -82.969407  \n",
            "2014-10-31   -94.592711  \n",
            "2014-11-01   -93.559629  \n",
            "2014-11-02   -92.688153  \n",
            "2014-11-03   -90.380478  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Features Avancées de Marché\n",
        "# -----------------------------------------------\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def add_market_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ajoute des features universelles de marché applicables à tout type d'actif.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame avec les données OHLCV\n",
        "    Returns:\n",
        "        DataFrame enrichi avec les features de marché\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = data.copy()\n",
        "\n",
        "        # 1. Volatilité relative\n",
        "        for window in [5, 10, 20]:\n",
        "            # Range normalisé\n",
        "            df[f'Normalized_Range_{window}d'] = (\n",
        "                (df['High'].rolling(window).max() - df['Low'].rolling(window).min()) /\n",
        "                df['Close'].rolling(window).mean()\n",
        "            ) * 100\n",
        "\n",
        "            # Volatilité de Parkinson (basée sur High-Low)\n",
        "            df[f'Parkinson_Volatility_{window}d'] = (\n",
        "                np.sqrt(\n",
        "                    1 / (4 * window * np.log(2)) *\n",
        "                    ((np.log(df['High'] / df['Low'])) ** 2)\n",
        "                ).rolling(window).mean() * np.sqrt(252)\n",
        "            )\n",
        "\n",
        "        # 2. Volume patterns\n",
        "        df['Volume_Price_Impact'] = (\n",
        "            (df['Close'] - df['Open']).abs() /\n",
        "            (df['Volume'] * df['Close'])\n",
        "        ).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        df['Volume_Distribution'] = (\n",
        "            df['Volume'] / df['Volume'].rolling(20).mean()\n",
        "        )\n",
        "\n",
        "        # 3. Momentum et accélération\n",
        "        df['Price_Acceleration'] = df['Close'].pct_change().diff()\n",
        "        df['Volume_Acceleration'] = df['Volume'].pct_change().diff()\n",
        "\n",
        "        # 4. Patterns de trading\n",
        "        df['Upper_Shadow'] = (\n",
        "            (df['High'] - df[['Open', 'Close']].max(axis=1)) /\n",
        "            df['Close']\n",
        "        ) * 100\n",
        "\n",
        "        df['Lower_Shadow'] = (\n",
        "            (df[['Open', 'Close']].min(axis=1) - df['Low']) /\n",
        "            df['Close']\n",
        "        ) * 100\n",
        "\n",
        "        df['Body_Size'] = (\n",
        "            (df['Close'] - df['Open']).abs() /\n",
        "            df['Close']\n",
        "        ) * 100\n",
        "\n",
        "        # 5. Gap analysis\n",
        "        df['Daily_Gap'] = (\n",
        "            (df['Open'] - df['Close'].shift(1)) /\n",
        "            df['Close'].shift(1)\n",
        "        ) * 100\n",
        "\n",
        "        # 6. Momentum confirmations\n",
        "        df['Price_Volume_Trend'] = (\n",
        "            df['Close'].pct_change() *\n",
        "            df['Volume'] /\n",
        "            df['Volume'].rolling(20).mean()\n",
        "        )\n",
        "\n",
        "        # 7. Support/Resistance proxies\n",
        "        for window in [10, 20, 50]:\n",
        "            # Distance from moving average\n",
        "            df[f'MA_Distance_{window}d'] = (\n",
        "                (df['Close'] - df['Close'].rolling(window).mean()) /\n",
        "                df['Close'].rolling(window).mean()\n",
        "            ) * 100\n",
        "\n",
        "            # Rolling support/resistance levels\n",
        "            df[f'Support_Distance_{window}d'] = (\n",
        "                (df['Close'] - df['Low'].rolling(window).min()) /\n",
        "                df['Low'].rolling(window).min()\n",
        "            ) * 100\n",
        "\n",
        "            df[f'Resistance_Distance_{window}d'] = (\n",
        "                (df['High'].rolling(window).max() - df['Close']) /\n",
        "                df['Close']\n",
        "            ) * 100\n",
        "\n",
        "        # Nettoyage des valeurs infinies ou NA\n",
        "        df = df.replace([np.inf, -np.inf], np.nan)\n",
        "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        logger.info(f\"Features de marché ajoutées: {[col for col in df.columns if col not in data.columns]}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur lors de l'ajout des features de marché: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "def add_extended_market_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ajoute des features de marché plus avancées.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame avec les données OHLCV et features de base\n",
        "    Returns:\n",
        "        DataFrame enrichi avec les features étendues\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = data.copy()\n",
        "\n",
        "        # 1. Consolidation/Expansion patterns\n",
        "        for window in [5, 10, 20]:\n",
        "            # Volatility expansion/contraction\n",
        "            df[f'Volatility_Change_{window}d'] = (\n",
        "                df['Close'].rolling(window).std() /\n",
        "                df['Close'].rolling(window).std().shift(window)\n",
        "            ) - 1\n",
        "\n",
        "            # Volume expansion/contraction\n",
        "            df[f'Volume_Change_{window}d'] = (\n",
        "                df['Volume'].rolling(window).mean() /\n",
        "                df['Volume'].rolling(window).mean().shift(window)\n",
        "            ) - 1\n",
        "\n",
        "            # Price range expansion/contraction\n",
        "            df[f'Range_Change_{window}d'] = (\n",
        "                (df['High'].rolling(window).max() - df['Low'].rolling(window).min()) /\n",
        "                (df['High'].rolling(window).max().shift(window) -\n",
        "                 df['Low'].rolling(window).min().shift(window))\n",
        "            ) - 1\n",
        "\n",
        "        # 2. Trend strength indicators\n",
        "        df['ADX_Trend_Strength'] = df['ADX'] / 100 if 'ADX' in df.columns else np.nan\n",
        "        df['Trend_Intensity'] = (\n",
        "            df['Close'].diff().rolling(20).apply(lambda x: (x > 0).sum() / 20)\n",
        "        )\n",
        "\n",
        "        # 3. Volume profile\n",
        "        df['Volume_Profile_High'] = (\n",
        "            df['Volume'] * (df['High'] / df['Close'] - 1)\n",
        "        ).rolling(10).mean()\n",
        "\n",
        "        df['Volume_Profile_Low'] = (\n",
        "            df['Volume'] * (1 - df['Low'] / df['Close'])\n",
        "        ).rolling(10).mean()\n",
        "\n",
        "        # 4. Price efficiency\n",
        "        for window in [5, 10, 20]:\n",
        "            df[f'Price_Efficiency_{window}d'] = (\n",
        "                (df['Close'] - df['Close'].shift(window)).abs() /\n",
        "                (df['High'].rolling(window).max() - df['Low'].rolling(window).min())\n",
        "            )\n",
        "\n",
        "        # Nettoyage des valeurs infinies ou NA\n",
        "        df = df.replace([np.inf, -np.inf], np.nan)\n",
        "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        logger.info(f\"Features étendues ajoutées: {[col for col in df.columns if col not in data.columns]}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur lors de l'ajout des features étendues: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "# Étape 1: Télécharger les données\n",
        "#df = download_and_prepare_data(config)\n",
        "\n",
        "# Étape 2: Ajouter les indicateurs techniques\n",
        "#df = add_technical_indicators(df, config)\n",
        "\n",
        "# Étape 3: Ajouter les features de marché de base\n",
        "df = add_market_features(df)\n",
        "\n",
        "# Étape 4: Ajouter les features de marché étendues\n",
        "df_final = add_extended_market_features(df)\n",
        "\n",
        "# Afficher les informations\n",
        "print(\"\\nColonnes disponibles :\")\n",
        "print(df_final.columns.tolist())\n",
        "\n",
        "print(\"\\nAperçu des données :\")\n",
        "print(df_final.head())\n",
        "\n",
        "# Optionnel : Sauvegarder les données\n",
        "#df_final.to_csv('analysis_results.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be8jMshICd2c",
        "outputId": "02051da4-5d01-4b90-9801-dbe8094414fe"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-271-36db7bbd653f>:100: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Colonnes disponibles :\n",
            "['Close', 'High', 'Low', 'Open', 'Volume', 'Year', 'Month', 'Day', 'Day_of_Week', 'Normalized_Range_5d', 'Parkinson_Volatility_5d', 'Normalized_Range_10d', 'Parkinson_Volatility_10d', 'Normalized_Range_20d', 'Parkinson_Volatility_20d', 'Volume_Price_Impact', 'Volume_Distribution', 'Price_Acceleration', 'Volume_Acceleration', 'Upper_Shadow', 'Lower_Shadow', 'Body_Size', 'Daily_Gap', 'Price_Volume_Trend', 'MA_Distance_10d', 'Support_Distance_10d', 'Resistance_Distance_10d', 'MA_Distance_20d', 'Support_Distance_20d', 'Resistance_Distance_20d', 'MA_Distance_50d', 'Support_Distance_50d', 'Resistance_Distance_50d', 'Volatility_Change_5d', 'Volume_Change_5d', 'Range_Change_5d', 'Volatility_Change_10d', 'Volume_Change_10d', 'Range_Change_10d', 'Volatility_Change_20d', 'Volume_Change_20d', 'Range_Change_20d', 'ADX_Trend_Strength', 'Trend_Intensity', 'Volume_Profile_High', 'Volume_Profile_Low', 'Price_Efficiency_5d', 'Price_Efficiency_10d', 'Price_Efficiency_20d']\n",
            "\n",
            "Aperçu des données :\n",
            "                 Close        High         Low        Open    Volume  Year  \\\n",
            "Date                                                                         \n",
            "2014-09-17  457.334015  468.174011  452.421997  465.864014  21056800  2014   \n",
            "2014-09-18  424.440002  456.859985  413.104004  456.859985  34483200  2014   \n",
            "2014-09-19  394.795990  427.834991  384.532013  424.102997  37919700  2014   \n",
            "2014-09-20  408.903992  423.295990  389.882996  394.673004  36863600  2014   \n",
            "2014-09-21  398.821014  412.425995  393.181000  408.084991  26580100  2014   \n",
            "\n",
            "            Month  Day  Day_of_Week  Normalized_Range_5d  ...  \\\n",
            "Date                                                      ...   \n",
            "2014-09-17      9   17            2            20.064818  ...   \n",
            "2014-09-18      9   18            3            20.064818  ...   \n",
            "2014-09-19      9   19            4            20.064818  ...   \n",
            "2014-09-20      9   20            5            20.064818  ...   \n",
            "2014-09-21      9   21            6            20.064818  ...   \n",
            "\n",
            "            Volatility_Change_20d  Volume_Change_20d  Range_Change_20d  \\\n",
            "Date                                                                     \n",
            "2014-09-17                -0.4888          -0.238238         -0.490066   \n",
            "2014-09-18                -0.4888          -0.238238         -0.490066   \n",
            "2014-09-19                -0.4888          -0.238238         -0.490066   \n",
            "2014-09-20                -0.4888          -0.238238         -0.490066   \n",
            "2014-09-21                -0.4888          -0.238238         -0.490066   \n",
            "\n",
            "            ADX_Trend_Strength  Trend_Intensity  Volume_Profile_High  \\\n",
            "Date                                                                   \n",
            "2014-09-17                 NaN              0.3         1.166338e+06   \n",
            "2014-09-18                 NaN              0.3         1.166338e+06   \n",
            "2014-09-19                 NaN              0.3         1.166338e+06   \n",
            "2014-09-20                 NaN              0.3         1.166338e+06   \n",
            "2014-09-21                 NaN              0.3         1.166338e+06   \n",
            "\n",
            "            Volume_Profile_Low  Price_Efficiency_5d  Price_Efficiency_10d  \\\n",
            "Date                                                                        \n",
            "2014-09-17       914406.472971             0.762941              0.799331   \n",
            "2014-09-18       914406.472971             0.762941              0.799331   \n",
            "2014-09-19       914406.472971             0.762941              0.799331   \n",
            "2014-09-20       914406.472971             0.762941              0.799331   \n",
            "2014-09-21       914406.472971             0.762941              0.799331   \n",
            "\n",
            "            Price_Efficiency_20d  \n",
            "Date                              \n",
            "2014-09-17              0.722989  \n",
            "2014-09-18              0.722989  \n",
            "2014-09-19              0.722989  \n",
            "2014-09-20              0.722989  \n",
            "2014-09-21              0.722989  \n",
            "\n",
            "[5 rows x 49 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-271-36db7bbd653f>:166: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Définition des Features et Division des Données\n",
        "# -----------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Any, Tuple, List\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def add_scale_invariant_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ajoute des features invariantes à l'échelle des prix.\n",
        "    Utilise principalement des variations relatives et des ratios.\n",
        "    \"\"\"\n",
        "    df = data.copy()\n",
        "\n",
        "    # 1. Returns sur différentes périodes\n",
        "    for period in [1, 3, 7, 14, 30]:\n",
        "        # Returns arithmétiques\n",
        "        df[f'Return_{period}d'] = df['Close'].pct_change(period) * 100\n",
        "        # Returns logarithmiques\n",
        "        df[f'Log_Return_{period}d'] = np.log(df['Close'] / df['Close'].shift(period))\n",
        "\n",
        "    # 2. Ratios de prix (déjà invariants à l'échelle)\n",
        "    df['High_Low_Ratio'] = df['High'] / df['Low']\n",
        "    df['Close_Open_Ratio'] = df['Close'] / df['Open']\n",
        "\n",
        "    # 3. Ratios de volume\n",
        "    df['Volume_Price_Ratio'] = df['Volume'] / df['Close']  # Volume/Price ratio\n",
        "    df['Relative_Volume'] = df['Volume'] / df['Volume'].rolling(30).mean()  # Volume relatif\n",
        "\n",
        "    # 4. Momentum relatif\n",
        "    for period in [7, 14, 30]:\n",
        "        df[f'Price_ROC_{period}d'] = (df['Close'] - df['Close'].shift(period)) / df['Close'].shift(period) * 100\n",
        "        df[f'Volume_ROC_{period}d'] = (df['Volume'] - df['Volume'].shift(period)) / df['Volume'].shift(period) * 100\n",
        "\n",
        "    # 5. Volatilité relative\n",
        "    for period in [7, 14, 30]:\n",
        "        # Volatilité basée sur les returns\n",
        "        df[f'Volatility_{period}d'] = df['Return_1d'].rolling(window=period).std()\n",
        "        # Volatilité normalisée par le prix\n",
        "        df[f'Normalized_TR_{period}d'] = (df['High'] - df['Low']) / df['Close'].rolling(period).mean() * 100\n",
        "\n",
        "    # 6. Indicateurs techniques normalisés\n",
        "    # Normaliser Bollinger Bands\n",
        "    if 'Bollinger_Middle' in df.columns:\n",
        "        df['BB_Width'] = (df['Bollinger_Upper'] - df['Bollinger_Lower']) / df['Bollinger_Middle'] * 100\n",
        "        df['BB_Position'] = (df['Close'] - df['Bollinger_Lower']) / (df['Bollinger_Upper'] - df['Bollinger_Lower'])\n",
        "\n",
        "    # 7. Position relative des prix\n",
        "    for period in [30, 90]:\n",
        "        df[f'Price_Position_{period}d'] = (df['Close'] - df['Low'].rolling(period).min()) / \\\n",
        "                                        (df['High'].rolling(period).max() - df['Low'].rolling(period).min())\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_target_scaling(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Prépare la configuration pour le scaling de la cible.\n",
        "    \"\"\"\n",
        "    config['preprocessing']['target_transform'] = {\n",
        "        'method': 'returns',  # ['returns', 'log_returns']\n",
        "        'prediction_mode': 'cumulative',  # ['cumulative', 'direct']\n",
        "        'periods': list(range(1, 31)),  # Prédiction à 30 jours\n",
        "        'use_log_returns': True\n",
        "    }\n",
        "\n",
        "    # Modification de la configuration du modèle pour prendre en compte les returns\n",
        "    config['model']['architecture'].update({\n",
        "        'output_mode': 'returns',\n",
        "        'return_scaling': 'log' if config['preprocessing']['target_transform']['use_log_returns'] else 'standard'\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def transform_target_to_returns(data: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforme la variable cible en returns pour la rendre invariante à l'échelle.\n",
        "    \"\"\"\n",
        "    df = data.copy()\n",
        "    target_config = config['preprocessing']['target_transform']\n",
        "\n",
        "    if target_config['method'] == 'returns':\n",
        "        # Calculer les returns pour chaque horizon de prédiction\n",
        "        for period in target_config['periods']:\n",
        "            if target_config['use_log_returns']:\n",
        "                df[f'Target_Return_{period}d'] = np.log(df['Close'].shift(-period) / df['Close'])\n",
        "            else:\n",
        "                df[f'Target_Return_{period}d'] = (df['Close'].shift(-period) / df['Close'] - 1) * 100\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_features_and_split(data: pd.DataFrame, config: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str], str]:\n",
        "    \"\"\"\n",
        "    Prépare les features et divise les données pour l'entraînement.\n",
        "    \"\"\"\n",
        "    def _validate_split_ratios(ratios: Dict[str, float]) -> None:\n",
        "        total = sum(ratios.values())\n",
        "        if not np.isclose(total, 1.0):\n",
        "            raise ValueError(f\"Les ratios doivent sommer à 1.0 (actuel: {total})\")\n",
        "\n",
        "    try:\n",
        "        # 1. Préparation des données de base\n",
        "        df = data.copy()\n",
        "\n",
        "        # 2. Création des returns (cible)\n",
        "        # Calcul des returns pour différentes périodes\n",
        "        for period in range(1, 31):\n",
        "            df[f'Target_Return_{period}d'] = df['Close'].pct_change(period) * 100\n",
        "            if config.get('preprocessing', {}).get('target_transform', {}).get('use_log_returns', True):\n",
        "                df[f'Target_Log_Return_{period}d'] = np.log(df['Close']/df['Close'].shift(period)) * 100\n",
        "\n",
        "        # 3. Features invariantes à l'échelle\n",
        "        df = add_scale_invariant_features(df)\n",
        "\n",
        "        # 4. Features avancées de marché\n",
        "        advanced_config = config.get('feature_engineering', {}).get('advanced_features', {})\n",
        "\n",
        "        if advanced_config.get('use_market_features', True):\n",
        "            df = add_market_features(df)\n",
        "            logger.info(\"Features de marché universelles ajoutées\")\n",
        "\n",
        "        if advanced_config.get('use_extended_features', True):\n",
        "            df = add_extended_market_features(df)\n",
        "            logger.info(\"Features de marché étendues ajoutées\")\n",
        "\n",
        "        # 5. Sélection des features\n",
        "        feature_config = config.get('feature_engineering', {})\n",
        "        features = []\n",
        "\n",
        "        # Features de prix et techniques\n",
        "        for category in ['market_features', 'extended_features']:\n",
        "            if advanced_config.get(f'use_{category}', True):\n",
        "                features.extend([col for col in df.columns if col.startswith(tuple([\n",
        "                    'Return_', 'Log_Return_', 'Normalized_', 'Volume_', 'Price_',\n",
        "                    'MA_', 'Support_', 'Resistance_', 'Volatility_'\n",
        "                ]))])\n",
        "\n",
        "        # Features temporelles\n",
        "        temp_features = feature_config.get('temporal_features', {})\n",
        "        temp_cols = {'year': 'Year', 'month': 'Month', 'day': 'Day', 'day_of_week': 'Day_of_Week'}\n",
        "        features.extend([col for key, col in temp_cols.items() if temp_features.get(f'use_{key}', False)])\n",
        "\n",
        "        # 6. Target selection\n",
        "        target = 'Target_Return_30d'  # Ou Target_Log_Return_30d selon la config\n",
        "        if config.get('preprocessing', {}).get('target_transform', {}).get('use_log_returns', True):\n",
        "            target = 'Target_Log_Return_30d'\n",
        "\n",
        "        # Retrait de la cible des features\n",
        "        features = [f for f in features if not f.startswith('Target_')]\n",
        "\n",
        "        # 7. Gestion des données manquantes\n",
        "        missing_config = config.get('preprocessing', {}).get('handle_missing', {})\n",
        "        method = missing_config.get('method', 'drop')\n",
        "\n",
        "        if method == 'drop':\n",
        "            df = df.dropna(subset=features + [target])\n",
        "        elif method == 'fill_forward':\n",
        "            df = df.ffill().bfill()  # Utilisation de ffill() et bfill() au lieu de fillna\n",
        "        elif method == 'interpolate':\n",
        "            df = df.interpolate(method=missing_config.get('interpolation_method', 'linear'))\n",
        "\n",
        "        # 8. Division des données avec chevauchement\n",
        "        split_config = config.get('data_split', {})\n",
        "        overlap_days = 30\n",
        "\n",
        "        n = len(df)\n",
        "        train_size = int(n * split_config.get('train_ratio', 0.8))\n",
        "        val_size = int(n * split_config.get('validation_ratio', 0.1))\n",
        "\n",
        "        train_data = df.iloc[:train_size]\n",
        "        val_data = df.iloc[train_size-overlap_days:train_size+val_size]\n",
        "        test_data = df.iloc[train_size+val_size-overlap_days:]\n",
        "\n",
        "        # Log des statistiques\n",
        "        logger.info(f\"\"\"\n",
        "        Division des données (avec chevauchement de {overlap_days} jours):\n",
        "        Train: {len(train_data)} observations ({train_data.index[0]} à {train_data.index[-1]})\n",
        "        Val  : {len(val_data)} observations ({val_data.index[0]} à {val_data.index[-1]})\n",
        "        Test : {len(test_data)} observations ({test_data.index[0]} à {test_data.index[-1]})\n",
        "\n",
        "        Statistiques des returns cibles ({target}):\n",
        "        Train: mean={train_data[target].mean():.2f}%, std={train_data[target].std():.2f}%\n",
        "        Val  : mean={val_data[target].mean():.2f}%, std={val_data[target].std():.2f}%\n",
        "        Test : mean={test_data[target].mean():.2f}%, std={test_data[target].std():.2f}%\n",
        "        \"\"\")\n",
        "\n",
        "        return train_data, val_data, test_data, features, target\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur lors de la préparation des données: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Pipeline complet\n",
        "# 1. Télécharger les données\n",
        "#df = download_and_prepare_data(config)\n",
        "\n",
        "# 2. Ajouter les indicateurs techniques\n",
        "#df = add_technical_indicators(df, config)\n",
        "\n",
        "# 3. Ajouter les features de marché\n",
        "#df = add_market_features(df)\n",
        "#df = add_extended_market_features(df)\n",
        "\n",
        "# 4. Configurer le scaling de la cible\n",
        "config = prepare_target_scaling(config)\n",
        "\n",
        "# 5. Transformer la cible en returns\n",
        "df = transform_target_to_returns(df, config)\n",
        "\n",
        "# 6. Préparer les features et diviser les données\n",
        "train_data, val_data, test_data, features, target = prepare_features_and_split(df, config)\n",
        "\n",
        "# Afficher les informations\n",
        "print(\"\\nFeatures sélectionnées :\")\n",
        "print(features)\n",
        "\n",
        "print(f\"\\nVariable cible : {target}\")\n",
        "\n",
        "print(\"\\nDimensions des ensembles :\")\n",
        "print(f\"Train : {train_data.shape}\")\n",
        "print(f\"Validation : {val_data.shape}\")\n",
        "print(f\"Test : {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_migfNpKD6a",
        "outputId": "a5c794ad-5318-4eaf-dc99-7b555eba9e43"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-271-36db7bbd653f>:100: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Features sélectionnées :\n",
            "['Normalized_Range_5d', 'Normalized_Range_10d', 'Normalized_Range_20d', 'Volume_Price_Impact', 'Volume_Distribution', 'Price_Acceleration', 'Volume_Acceleration', 'Price_Volume_Trend', 'MA_Distance_10d', 'Support_Distance_10d', 'Resistance_Distance_10d', 'MA_Distance_20d', 'Support_Distance_20d', 'Resistance_Distance_20d', 'MA_Distance_50d', 'Support_Distance_50d', 'Resistance_Distance_50d', 'Return_1d', 'Log_Return_1d', 'Return_3d', 'Log_Return_3d', 'Return_7d', 'Log_Return_7d', 'Return_14d', 'Log_Return_14d', 'Return_30d', 'Log_Return_30d', 'Volume_Price_Ratio', 'Price_ROC_7d', 'Volume_ROC_7d', 'Price_ROC_14d', 'Volume_ROC_14d', 'Price_ROC_30d', 'Volume_ROC_30d', 'Volatility_7d', 'Normalized_TR_7d', 'Volatility_14d', 'Normalized_TR_14d', 'Volatility_30d', 'Normalized_TR_30d', 'Price_Position_30d', 'Price_Position_90d', 'Volatility_Change_5d', 'Volume_Change_5d', 'Volatility_Change_10d', 'Volume_Change_10d', 'Volatility_Change_20d', 'Volume_Change_20d', 'Volume_Profile_High', 'Volume_Profile_Low', 'Price_Efficiency_5d', 'Price_Efficiency_10d', 'Price_Efficiency_20d', 'Normalized_Range_5d', 'Normalized_Range_10d', 'Normalized_Range_20d', 'Volume_Price_Impact', 'Volume_Distribution', 'Price_Acceleration', 'Volume_Acceleration', 'Price_Volume_Trend', 'MA_Distance_10d', 'Support_Distance_10d', 'Resistance_Distance_10d', 'MA_Distance_20d', 'Support_Distance_20d', 'Resistance_Distance_20d', 'MA_Distance_50d', 'Support_Distance_50d', 'Resistance_Distance_50d', 'Return_1d', 'Log_Return_1d', 'Return_3d', 'Log_Return_3d', 'Return_7d', 'Log_Return_7d', 'Return_14d', 'Log_Return_14d', 'Return_30d', 'Log_Return_30d', 'Volume_Price_Ratio', 'Price_ROC_7d', 'Volume_ROC_7d', 'Price_ROC_14d', 'Volume_ROC_14d', 'Price_ROC_30d', 'Volume_ROC_30d', 'Volatility_7d', 'Normalized_TR_7d', 'Volatility_14d', 'Normalized_TR_14d', 'Volatility_30d', 'Normalized_TR_30d', 'Price_Position_30d', 'Price_Position_90d', 'Volatility_Change_5d', 'Volume_Change_5d', 'Volatility_Change_10d', 'Volume_Change_10d', 'Volatility_Change_20d', 'Volume_Change_20d', 'Volume_Profile_High', 'Volume_Profile_Low', 'Price_Efficiency_5d', 'Price_Efficiency_10d', 'Price_Efficiency_20d', 'Year', 'Month', 'Day', 'Day_of_Week']\n",
            "\n",
            "Variable cible : Target_Log_Return_30d\n",
            "\n",
            "Dimensions des ensembles :\n",
            "Train : (3020, 137)\n",
            "Validation : (407, 137)\n",
            "Test : (409, 137)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-271-36db7bbd653f>:166: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Normalisation des données\n",
        "# -----------------------------------------------\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def normalize_data(train_data: pd.DataFrame,\n",
        "                  val_data: pd.DataFrame,\n",
        "                  test_data: pd.DataFrame,\n",
        "                  features: list,\n",
        "                  config: dict,\n",
        "                  target: str = 'Close') -> tuple:\n",
        "    \"\"\"\n",
        "    Normalise les données financières en évitant la fuite de données.\n",
        "\n",
        "    Args:\n",
        "        train_data: Données d'entraînement\n",
        "        val_data: Données de validation\n",
        "        test_data: Données de test\n",
        "        features: Liste des features à utiliser\n",
        "        config: Configuration de la normalisation\n",
        "        target: Variable cible\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_norm, val_norm, test_norm, scalers)\n",
        "    \"\"\"\n",
        "    def clean_outliers(df: pd.DataFrame, cols: list, threshold: float, train_stats: dict = None) -> pd.DataFrame:\n",
        "        \"\"\"Nettoie les valeurs aberrantes des données numériques.\"\"\"\n",
        "        df_clean = df.copy()\n",
        "        for col in cols:\n",
        "            if df_clean[col].dtype in ['float64', 'int64']:\n",
        "                if train_stats is None:\n",
        "                    mean, std = df_clean[col].mean(), df_clean[col].std()\n",
        "                else:\n",
        "                    mean, std = train_stats[col]\n",
        "                df_clean[col] = df_clean[col].clip(\n",
        "                    lower=mean - threshold * std,\n",
        "                    upper=mean + threshold * std\n",
        "                )\n",
        "        return df_clean.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
        "\n",
        "    try:\n",
        "        # 1. Configuration et validation initiale\n",
        "        norm_config = config['preprocessing']['normalization']\n",
        "        normalize_target = norm_config.get('normalize_target', False)\n",
        "        exclude_features = set(norm_config.get('exclude_features', []))\n",
        "        logger.info(f\"Features exclues de la normalisation: {exclude_features}\")\n",
        "        logger.info(f\"Normalisation de la cible {'activée' if normalize_target else 'désactivée'}\")\n",
        "\n",
        "        # 2. Identification des features à normaliser\n",
        "        features_to_normalize = [\n",
        "            f for f in features\n",
        "            if (f not in exclude_features and\n",
        "                train_data[f].dtype in ['float64', 'int64'])\n",
        "        ]\n",
        "        logger.info(f\"Features à normaliser: {features_to_normalize}\")\n",
        "\n",
        "        # 3. Préparation des données\n",
        "        datasets = {\n",
        "            'train': train_data.copy(),\n",
        "            'val': val_data.copy(),\n",
        "            'test': test_data.copy()\n",
        "        }\n",
        "\n",
        "        # 4. Nettoyage des outliers si activé\n",
        "        if norm_config.get('clip', {}).get('enabled', False):\n",
        "            threshold = norm_config['clip'].get('threshold', 3)\n",
        "            logger.info(f\"Nettoyage des outliers activé (seuil: {threshold})\")\n",
        "\n",
        "            # Calcul des statistiques sur le train set\n",
        "            train_stats = {\n",
        "                col: (datasets['train'][col].mean(), datasets['train'][col].std())\n",
        "                for col in features_to_normalize\n",
        "            }\n",
        "\n",
        "            # Application du nettoyage\n",
        "            datasets['train'] = clean_outliers(datasets['train'], features_to_normalize, threshold)\n",
        "            datasets['val'] = clean_outliers(datasets['val'], features_to_normalize, threshold, train_stats)\n",
        "            datasets['test'] = clean_outliers(datasets['test'], features_to_normalize, threshold, train_stats)\n",
        "        else:\n",
        "            logger.info(\"Nettoyage des outliers désactivé\")\n",
        "\n",
        "        # 5. Sélection et validation du scaler\n",
        "        scaler_map = {\n",
        "            'standard': StandardScaler,\n",
        "            'robust': RobustScaler,\n",
        "            'minmax': MinMaxScaler\n",
        "        }\n",
        "        scaler_type = norm_config.get('type', 'standard').lower()\n",
        "        ScalerClass = scaler_map.get(scaler_type)\n",
        "        if not ScalerClass:\n",
        "            raise ValueError(f\"Type de normalisation invalide: {scaler_type}\")\n",
        "        logger.info(f\"Utilisation du scaler: {scaler_type}\")\n",
        "\n",
        "        # 6. Normalisation\n",
        "        scalers = {}\n",
        "        # Correction de l'expression ternaire\n",
        "        features_and_target = features_to_normalize + [target] if normalize_target else features_to_normalize\n",
        "        logger.info(f\"Features and target pour la normalisation: {features_and_target}\")\n",
        "\n",
        "        # Exclure manuellement la cible si nécessaire\n",
        "        if not normalize_target:\n",
        "            exclude_features.add(target)\n",
        "            logger.info(f\"Exclusion manuelle de la cible {target} de la normalisation.\")\n",
        "\n",
        "        normalized_datasets = {k: v.copy() for k, v in datasets.items()}\n",
        "\n",
        "        for feature in features_and_target:\n",
        "            if feature not in exclude_features:\n",
        "                # Création et fit du scaler sur le train set\n",
        "                scaler = ScalerClass()\n",
        "                train_values = datasets['train'][feature].values.reshape(-1, 1)\n",
        "                scaler.fit(train_values)\n",
        "                scalers[feature] = scaler\n",
        "\n",
        "                # Application de la transformation sur chaque dataset\n",
        "                for dataset_type in ['train', 'val', 'test']:\n",
        "                    values = datasets[dataset_type][feature].values.reshape(-1, 1)\n",
        "                    normalized_datasets[dataset_type][feature] = scaler.transform(values).flatten()\n",
        "\n",
        "                # Vérification des statistiques après normalisation\n",
        "                for dataset_type in ['train', 'val', 'test']:\n",
        "                    data = normalized_datasets[dataset_type][feature]\n",
        "                    logger.info(f\"{dataset_type} {feature} - Mean: {data.mean():.3f}, Std: {data.std():.3f}\")\n",
        "\n",
        "        # 7. Vérification finale de la qualité des données\n",
        "        for dataset_type, dataset in normalized_datasets.items():\n",
        "            # Vérification des valeurs manquantes et infinies\n",
        "            has_nan = dataset[features_and_target].isna().any().any()\n",
        "            has_inf = np.isinf(dataset[features_and_target].values).any()\n",
        "\n",
        "            if has_nan or has_inf:\n",
        "                raise ValueError(f\"Valeurs invalides détectées dans {dataset_type}\")\n",
        "\n",
        "            # Vérification des limites des valeurs normalisées\n",
        "            if norm_config['type'] == 'minmax':\n",
        "                if (dataset[features_and_target].values < 0).any() or (dataset[features_and_target].values > 1).any():\n",
        "                    raise ValueError(f\"Valeurs hors limites [0,1] détectées dans {dataset_type}\")\n",
        "\n",
        "        logger.info(\"Normalisation terminée avec succès\")\n",
        "        return (normalized_datasets['train'],\n",
        "                normalized_datasets['val'],\n",
        "                normalized_datasets['test'],\n",
        "                scalers)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur lors de la normalisation: {e}\")\n",
        "        raise\n",
        "\n",
        "# Pipeline complet\n",
        "# 1. Télécharger et préparer les données\n",
        "#df = download_and_prepare_data(config)\n",
        "#df = add_technical_indicators(df, config)\n",
        "#df = add_market_features(df)\n",
        "#df = add_extended_market_features(df)\n",
        "\n",
        "# 2. Préparer les features et diviser les données\n",
        "#train_data, val_data, test_data, features, target = prepare_features_and_split(df, config)\n",
        "\n",
        "# 3. Normaliser les données\n",
        "train_normalized, val_normalized, test_normalized, scalers = normalize_data(\n",
        "    train_data=train_data,\n",
        "    val_data=val_data,\n",
        "    test_data=test_data,\n",
        "    features=features,\n",
        "    config=config,\n",
        "    target=target\n",
        ")\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"\\nAperçu des données normalisées :\")\n",
        "print(\"\\nTrain :\")\n",
        "print(train_normalized[features].describe())\n",
        "\n",
        "print(\"\\nValidation :\")\n",
        "print(val_normalized[features].describe())\n",
        "\n",
        "print(\"\\nTest :\")\n",
        "print(test_normalized[features].describe())"
      ],
      "metadata": {
        "id": "5HDroLPJL2YP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1149f34f-563c-4493-c5d1-cc201524a30e"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aperçu des données normalisées :\n",
            "\n",
            "Train :\n",
            "       Normalized_Range_5d  Normalized_Range_10d  Normalized_Range_20d  \\\n",
            "count         3.020000e+03          3.020000e+03          3.020000e+03   \n",
            "mean         -8.470046e-17         -9.411162e-18          5.646697e-17   \n",
            "std           1.000166e+00          1.000166e+00          1.000166e+00   \n",
            "min          -1.359455e+00         -1.424767e+00         -1.540005e+00   \n",
            "25%          -6.756420e-01         -7.155990e-01         -7.248465e-01   \n",
            "50%          -2.682604e-01         -2.186006e-01         -2.202685e-01   \n",
            "75%           4.652737e-01          4.282251e-01          4.833896e-01   \n",
            "max           7.627660e+00          5.350257e+00          5.195962e+00   \n",
            "\n",
            "       Volume_Price_Impact  Volume_Distribution  Price_Acceleration  \\\n",
            "count         3.020000e+03         3.020000e+03        3.020000e+03   \n",
            "mean         -3.764465e-17        -2.070456e-16        1.260949e-17   \n",
            "std           1.000166e+00         1.000166e+00        1.000166e+00   \n",
            "min          -3.730412e-01        -2.258095e+00       -6.789836e+00   \n",
            "25%          -3.715556e-01        -6.105175e-01       -4.826513e-01   \n",
            "50%          -3.676135e-01        -1.783772e-01       -2.297518e-03   \n",
            "75%          -1.977015e-01         3.481505e-01        4.656377e-01   \n",
            "max           1.320203e+01         8.150377e+00        8.962537e+00   \n",
            "\n",
            "       Volume_Acceleration  Price_Volume_Trend  MA_Distance_10d  \\\n",
            "count         3.020000e+03         3020.000000      3020.000000   \n",
            "mean          1.066108e-17            0.000000         0.000000   \n",
            "std           1.000166e+00            1.000166         1.000166   \n",
            "min          -1.127673e+01          -13.133744        -6.047927   \n",
            "25%          -4.227718e-01           -0.263927        -0.487410   \n",
            "50%           1.677180e-02           -0.033327        -0.025970   \n",
            "75%           4.349525e-01            0.239226         0.500686   \n",
            "max           9.982823e+00           11.744288         7.535917   \n",
            "\n",
            "       Support_Distance_10d  ...  Volume_Change_20d  Volume_Profile_High  \\\n",
            "count          3.020000e+03  ...       3.020000e+03          3020.000000   \n",
            "mean           3.764465e-17  ...      -4.705581e-17             0.000000   \n",
            "std            1.000166e+00  ...       1.000166e+00             1.000166   \n",
            "min           -1.053928e+00  ...      -1.807251e+00            -0.585626   \n",
            "25%           -7.138895e-01  ...      -6.241870e-01            -0.583359   \n",
            "50%           -3.301757e-01  ...      -2.001561e-01            -0.377526   \n",
            "75%            4.278354e-01  ...       3.478691e-01             0.188956   \n",
            "max            8.265475e+00  ...       5.931999e+00             8.731497   \n",
            "\n",
            "       Volume_Profile_Low  Price_Efficiency_5d  Price_Efficiency_10d  \\\n",
            "count        3.020000e+03         3.020000e+03          3.020000e+03   \n",
            "mean        -3.764465e-17         1.741065e-16         -5.176139e-17   \n",
            "std          1.000166e+00         1.000166e+00          1.000166e+00   \n",
            "min         -6.069121e-01        -1.837792e+00         -1.813319e+00   \n",
            "25%         -6.040434e-01        -8.231270e-01         -8.497806e-01   \n",
            "50%         -4.009802e-01         4.685727e-02          6.239843e-02   \n",
            "75%          2.250838e-01         8.261819e-01          8.285605e-01   \n",
            "max          8.006339e+00         2.462679e+00          1.902066e+00   \n",
            "\n",
            "       Price_Efficiency_20d         Year        Month          Day  \\\n",
            "count          3.020000e+03  3020.000000  3020.000000  3020.000000   \n",
            "mean           7.999488e-17  2018.332450     6.656291    15.737748   \n",
            "std            1.000166e+00     2.393974     3.474758     8.793014   \n",
            "min           -1.816507e+00  2014.000000     1.000000     1.000000   \n",
            "25%           -8.652471e-01  2016.000000     4.000000     8.000000   \n",
            "50%            5.012921e-02  2018.000000     7.000000    16.000000   \n",
            "75%            8.429548e-01  2020.000000    10.000000    23.000000   \n",
            "max            1.847269e+00  2022.000000    12.000000    31.000000   \n",
            "\n",
            "       Day_of_Week  \n",
            "count  3020.000000  \n",
            "mean      3.000000  \n",
            "std       1.999503  \n",
            "min       0.000000  \n",
            "25%       1.000000  \n",
            "50%       3.000000  \n",
            "75%       5.000000  \n",
            "max       6.000000  \n",
            "\n",
            "[8 rows x 110 columns]\n",
            "\n",
            "Validation :\n",
            "       Normalized_Range_5d  Normalized_Range_10d  Normalized_Range_20d  \\\n",
            "count           407.000000            407.000000            407.000000   \n",
            "mean             -0.563515             -0.632771             -0.704181   \n",
            "std               0.509081              0.527126              0.530404   \n",
            "min              -1.238628             -1.281047             -1.508373   \n",
            "25%              -0.891090             -0.971634             -1.051762   \n",
            "50%              -0.674488             -0.746812             -0.861439   \n",
            "75%              -0.386848             -0.505271             -0.549246   \n",
            "max               2.369209              1.713934              0.834307   \n",
            "\n",
            "       Volume_Price_Impact  Volume_Distribution  Price_Acceleration  \\\n",
            "count           407.000000           407.000000          407.000000   \n",
            "mean             -0.371146            -0.092843           -0.000220   \n",
            "std               0.001627             0.981915            0.578595   \n",
            "min              -0.373039            -1.746816           -2.229014   \n",
            "25%              -0.372402            -0.788356           -0.291299   \n",
            "50%              -0.371659            -0.205929            0.010399   \n",
            "75%              -0.370211             0.327168            0.318057   \n",
            "max              -0.363191             4.789994            1.962256   \n",
            "\n",
            "       Volume_Acceleration  Price_Volume_Trend  MA_Distance_10d  \\\n",
            "count           407.000000          407.000000       407.000000   \n",
            "mean             -0.000771            0.021465         0.073219   \n",
            "std               1.132001            0.611101         0.625222   \n",
            "min              -6.054094           -2.956050        -1.510487   \n",
            "25%              -0.626480           -0.175711        -0.269542   \n",
            "50%               0.008716           -0.060425        -0.036990   \n",
            "75%               0.638688            0.117566         0.313122   \n",
            "max               4.948299            4.854117         2.770497   \n",
            "\n",
            "       Support_Distance_10d  ...  Volume_Change_20d  Volume_Profile_High  \\\n",
            "count            407.000000  ...         407.000000           407.000000   \n",
            "mean              -0.383921  ...          -0.229120            -0.204260   \n",
            "std                0.698546  ...           0.718745             0.236200   \n",
            "min               -1.053928  ...          -1.637756            -0.533995   \n",
            "25%               -0.833244  ...          -0.706971            -0.382903   \n",
            "50%               -0.594844  ...          -0.226830            -0.250021   \n",
            "75%               -0.252424  ...           0.120186            -0.095200   \n",
            "max                3.170879  ...           2.185217             0.931459   \n",
            "\n",
            "       Volume_Profile_Low  Price_Efficiency_5d  Price_Efficiency_10d  \\\n",
            "count          407.000000           407.000000            407.000000   \n",
            "mean            -0.186794            -0.204777             -0.162545   \n",
            "std              0.331512             0.983481              0.998312   \n",
            "min             -0.528100            -1.836713             -1.809247   \n",
            "25%             -0.393301            -1.053901             -1.035546   \n",
            "50%             -0.279844            -0.213081             -0.198029   \n",
            "75%             -0.083732             0.577302              0.718472   \n",
            "max              1.597136             1.795599              1.893308   \n",
            "\n",
            "       Price_Efficiency_20d         Year       Month         Day  Day_of_Week  \n",
            "count            407.000000   407.000000  407.000000  407.000000        407.0  \n",
            "mean              -0.093973  2022.916462    6.965602   15.805897          3.0  \n",
            "std                1.002846     0.310568    3.664340    8.925480          2.0  \n",
            "min               -1.806677  2022.000000    1.000000    1.000000          0.0  \n",
            "25%               -0.959879  2023.000000    4.000000    8.000000          1.0  \n",
            "50%               -0.117322  2023.000000    7.000000   16.000000          3.0  \n",
            "75%                0.757223  2023.000000   10.000000   24.000000          5.0  \n",
            "max                1.820267  2024.000000   12.000000   31.000000          6.0  \n",
            "\n",
            "[8 rows x 110 columns]\n",
            "\n",
            "Test :\n",
            "       Normalized_Range_5d  Normalized_Range_10d  Normalized_Range_20d  \\\n",
            "count           409.000000            409.000000            409.000000   \n",
            "mean             -0.287130             -0.353430             -0.439286   \n",
            "std               0.483444              0.471184              0.453225   \n",
            "min              -1.095434             -1.161104             -1.210787   \n",
            "25%              -0.615434             -0.668712             -0.728864   \n",
            "50%              -0.369467             -0.466079             -0.544254   \n",
            "75%              -0.027639             -0.114275             -0.264844   \n",
            "max               2.057855              1.545924              0.880270   \n",
            "\n",
            "       Volume_Price_Impact  Volume_Distribution  Price_Acceleration  \\\n",
            "count           409.000000           409.000000          409.000000   \n",
            "mean             -0.371603            -0.025093           -0.000576   \n",
            "std               0.001140             1.076102            0.732203   \n",
            "min              -0.373042            -1.844557           -2.702617   \n",
            "25%              -0.372499            -0.809074           -0.464140   \n",
            "50%              -0.371871            -0.094952           -0.016161   \n",
            "75%              -0.370987             0.552568            0.473082   \n",
            "max              -0.366882             5.251417            3.292078   \n",
            "\n",
            "       Volume_Acceleration  Price_Volume_Trend  MA_Distance_10d  \\\n",
            "count           409.000000          409.000000       409.000000   \n",
            "mean              0.002880           -0.001796         0.059109   \n",
            "std               1.340479            0.724827         0.683762   \n",
            "min              -5.695505           -3.686478        -2.346946   \n",
            "25%              -0.693074           -0.253446        -0.388748   \n",
            "50%              -0.034090           -0.046111        -0.005986   \n",
            "75%               0.758808            0.186071         0.485385   \n",
            "max               4.881641            4.720728         2.709140   \n",
            "\n",
            "       Support_Distance_10d  ...  Volume_Change_20d  Volume_Profile_High  \\\n",
            "count            409.000000  ...         409.000000           409.000000   \n",
            "mean              -0.172883  ...          -0.013033             0.420751   \n",
            "std                0.644620  ...           0.864396             0.644553   \n",
            "min               -1.053928  ...          -1.433254            -0.324246   \n",
            "25%               -0.662646  ...          -0.514546            -0.049684   \n",
            "50%               -0.320446  ...          -0.171665             0.237980   \n",
            "75%                0.170015  ...           0.261246             0.643406   \n",
            "max                2.455949  ...           3.025022             2.314194   \n",
            "\n",
            "       Volume_Profile_Low  Price_Efficiency_5d  Price_Efficiency_10d  \\\n",
            "count          409.000000           409.000000            409.000000   \n",
            "mean             0.541250            -0.116740             -0.160950   \n",
            "std              0.840379             0.974716              0.984202   \n",
            "min             -0.322999            -1.818045             -1.812482   \n",
            "25%             -0.028851            -0.893129             -0.996150   \n",
            "50%              0.237673            -0.024927             -0.165493   \n",
            "75%              0.883833             0.717108              0.645730   \n",
            "max              4.278976             1.670686              1.811528   \n",
            "\n",
            "       Price_Efficiency_20d         Year       Month         Day  Day_of_Week  \n",
            "count            409.000000   409.000000  409.000000  409.000000    409.00000  \n",
            "mean              -0.126214  2023.977995    6.633252   15.650367      3.00000  \n",
            "std                0.979839     0.323893    3.722279    8.752451      1.99632  \n",
            "min               -1.813002  2023.000000    1.000000    1.000000      0.00000  \n",
            "25%               -0.878101  2024.000000    3.000000    8.000000      1.00000  \n",
            "50%               -0.251471  2024.000000    7.000000   15.000000      3.00000  \n",
            "75%                0.666930  2024.000000   10.000000   23.000000      5.00000  \n",
            "max                1.754310  2025.000000   12.000000   31.000000      6.00000  \n",
            "\n",
            "[8 rows x 110 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Utilisation du Pipeline Complet (Mis à Jour)\n",
        "# -----------------------------------------------\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def run_pipeline(config: Dict[str, Any]) -> tuple:\n",
        "    \"\"\"\n",
        "    Exécute le pipeline complet de préparation des données.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DÉBUT DU PIPELINE DE PRÉPARATION DES DONNÉES\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. Validation de la configuration\n",
        "    print(\"\\n1. VALIDATION DE LA CONFIGURATION\")\n",
        "    print(\"-\"*30)\n",
        "    required_keys = ['preprocessing', 'data', 'technical_indicators', 'feature_engineering']\n",
        "    missing_keys = [key for key in required_keys if key not in config]\n",
        "    if missing_keys:\n",
        "        raise ValueError(f\"Configuration invalide. Clés manquantes: {missing_keys}\")\n",
        "    print(f\"Configuration validée - Symbole: {config['data']['symbol']}\")\n",
        "\n",
        "    # 2. Téléchargement et préparation\n",
        "    print(\"\\n2. TÉLÉCHARGEMENT DES DONNÉES\")\n",
        "    print(\"-\"*30)\n",
        "    raw_data = download_and_prepare_data(config)\n",
        "    print(f\"Données brutes téléchargées:\")\n",
        "    print(f\"- Shape: {raw_data.shape}\")\n",
        "    print(f\"- Période: {raw_data.index[0]} à {raw_data.index[-1]}\")\n",
        "    print(f\"- Colonnes: {', '.join(raw_data.columns)}\")\n",
        "\n",
        "    # 3. Ajout des indicateurs techniques\n",
        "    print(\"\\n3. CALCUL DES INDICATEURS TECHNIQUES\")\n",
        "    print(\"-\"*30)\n",
        "    processed_data = add_technical_indicators(raw_data, config)\n",
        "    new_indicators = set(processed_data.columns) - set(raw_data.columns)\n",
        "    print(f\"Indicateurs ajoutés ({len(new_indicators)}):\")\n",
        "    print(f\"- {', '.join(sorted(new_indicators))}\")\n",
        "    print(f\"Lignes avant/après calcul des indicateurs: {len(raw_data)} → {len(processed_data)}\")\n",
        "    print(f\"Lignes retirées: {len(raw_data) - len(processed_data)} (période d'initialisation des indicateurs)\")\n",
        "\n",
        "    # 4. Préparation des features et division\n",
        "    print(\"\\n4. PRÉPARATION ET DIVISION DES DONNÉES\")\n",
        "    print(\"-\"*30)\n",
        "    train_data, val_data, test_data, features, target = prepare_features_and_split(\n",
        "        processed_data,\n",
        "        config\n",
        "    )\n",
        "    print(f\"Target sélectionnée: {target}\")\n",
        "    print(f\"Nombre de features: {len(features)}\")\n",
        "    print(f\"Distribution des données:\")\n",
        "    print(f\"- Train: {len(train_data)} ({len(train_data)/len(processed_data)*100:.1f}%)\")\n",
        "    print(f\"- Validation: {len(val_data)} ({len(val_data)/len(processed_data)*100:.1f}%)\")\n",
        "    print(f\"- Test: {len(test_data)} ({len(test_data)/len(processed_data)*100:.1f}%)\")\n",
        "\n",
        "    # 5. Normalisation\n",
        "    print(\"\\n5. NORMALISATION DES DONNÉES\")\n",
        "    print(\"-\"*30)\n",
        "    logger.info(\"Normalisation des données...\")\n",
        "    train_norm, val_norm, test_norm, scalers = normalize_data(\n",
        "        train_data,\n",
        "        val_data,\n",
        "        test_data,\n",
        "        features,\n",
        "        config,\n",
        "        target='Close'\n",
        "    )\n",
        "    print(\"Statistiques après normalisation (ensemble d'entraînement):\")\n",
        "    print(train_norm[features[:5]].describe().round(3))\n",
        "    print(\"\\nStatistiques de la variable cible (Close) après normalisation:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Train 'Close'    - Mean: {train_norm['Close'].mean():.3f}, Std: {train_norm['Close'].std():.3f}\")\n",
        "    print(f\"Validation 'Close' - Mean: {val_norm['Close'].mean():.3f}, Std: {val_norm['Close'].std():.3f}\")\n",
        "    print(f\"Test 'Close'      - Mean: {test_norm['Close'].mean():.3f}, Std: {test_norm['Close'].std():.3f}\")\n",
        "\n",
        "    # Vérification de la configuration de la normalisation\n",
        "    normalize_target = config.get('preprocessing', {}).get('normalization', {}).get('normalize_target', False)\n",
        "    logger.info(f\"Normalisation de la cible {'activée' if normalize_target else 'désactivée'}\")\n",
        "\n",
        "    # Vérifier que 'Close' n'est pas normalisé\n",
        "    if normalize_target:\n",
        "        logger.warning(\"La normalisation de la cible est activée, ce qui peut causer des incohérences.\")\n",
        "    else:\n",
        "        logger.info(\"'Close' est correctement exclu de la normalisation.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PIPELINE TERMINÉ AVEC SUCCÈS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\"\"\n",
        "Résumé final:\n",
        "- Nombre total d'observations: {len(processed_data)}\n",
        "- Nombre de features: {len(features)}\n",
        "- Features principales: {', '.join(features[:5])}...\n",
        "- Période couverte: {processed_data.index[0]} à {processed_data.index[-1]}\n",
        "- Train: {len(train_norm)} observations\n",
        "- Validation: {len(val_norm)} observations\n",
        "- Test: {len(test_norm)} observations\n",
        "\"\"\")\n",
        "\n",
        "    # Final Verification: Assurer que 'Close' n'est pas normalisé\n",
        "    if normalize_target:\n",
        "        # Si normalize_target est True, 'Close' est normalisé\n",
        "        if 'Close' in scalers:\n",
        "            logger.error(\"'Close' a été normalisé malgré la désactivation.\")\n",
        "        else:\n",
        "            logger.info(\"'Close' n'a pas été normalisé comme prévu.\")\n",
        "    else:\n",
        "        # Si normalize_target est False, 'Close' ne doit pas être normalisé\n",
        "        if 'Close' in scalers:\n",
        "            logger.error(\"'Close' a été normalisé alors que cela ne devrait pas être le cas.\")\n",
        "        else:\n",
        "            logger.info(\"'Close' n'a pas été normalisé comme prévu.\")\n",
        "\n",
        "    return train_norm, val_norm, test_norm, features, target, scalers\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Initialisation de la configuration WandB\n",
        "        #base_config = init_wandb_config(stock_symbol='BTC-USD', mode='disabled')\n",
        "        # S'assurer que 'normalize_target' est bien désactivé\n",
        "        base_config['preprocessing']['normalization']['normalize_target'] = False\n",
        "\n",
        "        # Exécution du pipeline\n",
        "        train_data, val_data, test_data, features, target, scalers = run_pipeline(base_config)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n\" + \"!\"*50)\n",
        "        print(\"ERREUR DANS LE PIPELINE\")\n",
        "        print(\"!\"*50)\n",
        "        logger.error(f\"Nature de l'erreur: {str(e)}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TdCD5oWJ8wX",
        "outputId": "ff99bd72-82ca-4100-92d1-817aacd3251e"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "DÉBUT DU PIPELINE DE PRÉPARATION DES DONNÉES\n",
            "==================================================\n",
            "\n",
            "1. VALIDATION DE LA CONFIGURATION\n",
            "------------------------------\n",
            "Configuration validée - Symbole: BTC-USD\n",
            "\n",
            "2. TÉLÉCHARGEMENT DES DONNÉES\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données brutes téléchargées:\n",
            "- Shape: (3776, 9)\n",
            "- Période: 2014-09-17 00:00:00 à 2025-01-17 00:00:00\n",
            "- Colonnes: Close, High, Low, Open, Volume, Year, Month, Day, Day_of_Week\n",
            "\n",
            "3. CALCUL DES INDICATEURS TECHNIQUES\n",
            "------------------------------\n",
            "Indicateurs ajoutés (26):\n",
            "- ADX, ATR, Bollinger_Lower, Bollinger_Middle, Bollinger_Upper, CCI, CMF, EMA_10, Ichimoku_A, Ichimoku_B, Ichimoku_Base_Line, Ichimoku_Conversion_Line, MACD, MACD_Diff, MACD_Signal, ROC, RSI, SMA_10, SMA_30, Stochastic_%D, Stochastic_%K, TRIX, VWAP, Volume_Change, Volume_MA_10, Williams_%R\n",
            "Lignes avant/après calcul des indicateurs: 3776 → 3733\n",
            "Lignes retirées: 43 (période d'initialisation des indicateurs)\n",
            "\n",
            "4. PRÉPARATION ET DIVISION DES DONNÉES\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-271-36db7bbd653f>:100: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n",
            "<ipython-input-271-36db7bbd653f>:166: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target sélectionnée: Target_Log_Return_30d\n",
            "Nombre de features: 114\n",
            "Distribution des données:\n",
            "- Train: 2986 (80.0%)\n",
            "- Validation: 403 (10.8%)\n",
            "- Test: 404 (10.8%)\n",
            "\n",
            "5. NORMALISATION DES DONNÉES\n",
            "------------------------------\n",
            "Statistiques après normalisation (ensemble d'entraînement):\n",
            "       Volume_MA_10  Volume_Change  Return_1d  Log_Return_1d  Return_3d\n",
            "count      2986.000       2986.000   2986.000       2986.000   2986.000\n",
            "mean          0.000         -0.000     -0.000          0.000     -0.000\n",
            "std           1.000          1.000      1.000          1.000      1.000\n",
            "min          -0.890         -2.526     -9.748        -12.051     -5.738\n",
            "25%          -0.884         -0.531     -0.405         -0.384     -0.477\n",
            "50%          -0.454         -0.170     -0.006          0.013     -0.017\n",
            "75%           0.681          0.327      0.408          0.420      0.487\n",
            "max           4.400         14.805      6.532          5.788      8.023\n",
            "\n",
            "Statistiques de la variable cible (Close) après normalisation:\n",
            "------------------------------\n",
            "Train 'Close'    - Mean: 13043.200, Std: 16228.949\n",
            "Validation 'Close' - Mean: 28353.447, Std: 6861.986\n",
            "Test 'Close'      - Mean: 66066.654, Std: 16280.567\n",
            "\n",
            "==================================================\n",
            "PIPELINE TERMINÉ AVEC SUCCÈS\n",
            "==================================================\n",
            "\n",
            "Résumé final:\n",
            "- Nombre total d'observations: 3733\n",
            "- Nombre de features: 114\n",
            "- Features principales: Volume_MA_10, Volume_Change, Return_1d, Log_Return_1d, Return_3d...\n",
            "- Période couverte: 2014-10-30 00:00:00 à 2025-01-17 00:00:00\n",
            "- Train: 2986 observations\n",
            "- Validation: 403 observations\n",
            "- Test: 404 observations\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Préparation des données pour le TFT\n",
        "# -----------------------------------------------\n",
        "\n",
        "import torch\n",
        "from typing import Tuple, Dict\n",
        "from pytorch_forecasting import TimeSeriesDataSet\n",
        "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def prepare_tft_datasets(\n",
        "    train_data: pd.DataFrame,\n",
        "    val_data: pd.DataFrame,\n",
        "    config: Dict\n",
        ") -> Tuple[TimeSeriesDataSet, torch.utils.data.DataLoader, TimeSeriesDataSet, torch.utils.data.DataLoader]:\n",
        "    \"\"\"\n",
        "    Prépare les datasets et dataloaders pour le TFT (Temporal Fusion Transformer).\n",
        "\n",
        "    Args:\n",
        "        train_data: DataFrame d'entraînement\n",
        "        val_data: DataFrame de validation\n",
        "        config: Configuration du modèle\n",
        "\n",
        "    Returns:\n",
        "        training: Dataset d'entraînement\n",
        "        train_dataloader: DataLoader d'entraînement\n",
        "        validation: Dataset de validation\n",
        "        val_dataloader: DataLoader de validation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Préparation des indices temporels et identifiants\n",
        "        for df in [train_data, val_data]:\n",
        "            df['time_idx'] = range(len(df))\n",
        "            df['series_id'] = config['data'].get('symbol', 'BTC-USD')\n",
        "            if config['feature_engineering']['temporal_features'].get('use_day_of_week', True):\n",
        "                df['Day_of_Week'] = df['Day_of_Week'].astype(str)\n",
        "\n",
        "        # 2. Configuration des features\n",
        "        excluded_cols = ['Year', 'Month', 'Day', 'series_id', 'time_idx']\n",
        "        categorical_cols = ['Day_of_Week'] if config['feature_engineering']['temporal_features'].get('use_day_of_week', True) else []\n",
        "\n",
        "        # Sélection des features numériques (tout sauf les colonnes exclues et catégorielles)\n",
        "        numeric_features = [col for col in train_data.columns\n",
        "                          if col not in excluded_cols + categorical_cols + ['Target_Return_30d']]\n",
        "\n",
        "        # 3. Paramètres du TimeSeriesDataSet\n",
        "        ts_params = config['pytorch_forecasting']['time_series_params']\n",
        "        training = TimeSeriesDataSet(\n",
        "            train_data,\n",
        "            time_idx=\"time_idx\",\n",
        "            target=\"Target_Return_30d\",  # Notre nouvelle cible\n",
        "            group_ids=[\"series_id\"],\n",
        "            max_encoder_length=ts_params['max_encoder_length'],\n",
        "            min_encoder_length=ts_params['min_encoder_length'],\n",
        "            max_prediction_length=ts_params['max_prediction_length'],\n",
        "            min_prediction_length=ts_params['min_prediction_length'],\n",
        "            static_categoricals=[],\n",
        "            time_varying_known_categoricals=categorical_cols,\n",
        "            time_varying_known_reals=[],\n",
        "            time_varying_unknown_reals=numeric_features,\n",
        "            target_normalizer=None,  # Déjà normalisé\n",
        "            categorical_encoders={col: NaNLabelEncoder() for col in categorical_cols}\n",
        "        )\n",
        "\n",
        "        # 4. Création des dataloaders\n",
        "        batch_size = config['pytorch_forecasting']['training_params']['batch_size']['value']\n",
        "        train_dataloader = training.to_dataloader(\n",
        "            train=True,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=0,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        validation = TimeSeriesDataSet.from_dataset(training, val_data, predict=True)\n",
        "        val_dataloader = validation.to_dataloader(\n",
        "            train=False,\n",
        "            batch_size=batch_size * 2,\n",
        "            num_workers=0,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # 5. Logging des informations importantes\n",
        "        logger.info(f\"\"\"\n",
        "        Configuration TFT:\n",
        "        - Features numériques: {len(numeric_features)}\n",
        "        - Features catégorielles: {categorical_cols}\n",
        "        - Longueur d'encodage: {ts_params['max_encoder_length']}\n",
        "        - Horizon de prédiction: {ts_params['max_prediction_length']}\n",
        "        - Batch size: {batch_size}\n",
        "        \"\"\")\n",
        "\n",
        "        return training, train_dataloader, validation, val_dataloader\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur dans la préparation des données TFT: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "try:\n",
        "    # 1. Initialisation de la configuration WandB\n",
        "    config = init_wandb_config(stock_symbol='BTC-USD', mode='disabled')\n",
        "\n",
        "    # 2. Préparation des datasets\n",
        "    training, train_dataloader, validation, val_dataloader = prepare_tft_datasets(\n",
        "        train_data=train_data,  # De l'étape précédente\n",
        "        val_data=val_data,      # De l'étape précédente\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # 3. Vérification rapide des dataloaders\n",
        "    batch = next(iter(train_dataloader))\n",
        "    x, y = batch\n",
        "    print(\"\\nDimensions d'un batch d'entraînement:\")\n",
        "    print(f\"Input dimensions: {x['encoder_cont'].shape}\")\n",
        "    print(f\"Target dimensions: {y[0].shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Erreur lors de la préparation des données: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "mYuzJi1uKpP-",
        "outputId": "cd187d53-5882-4e06-f333-052c4cafbc77"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialisation de la configuration WandB...\n",
            "Configuration WandB initialisée avec succès.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py:1207: UserWarning: If predicting, no randomization should be possible - setting stop_randomization=True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensions d'un batch d'entraînement:\n",
            "Input dimensions: torch.Size([32, 180, 161])\n",
            "Target dimensions: torch.Size([32, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def verify_dataloaders(train_dataloader, val_dataloader) -> None:\n",
        "    \"\"\"\n",
        "    Vérifie la structure et les statistiques des dataloaders TFT.\n",
        "\n",
        "    Args:\n",
        "        train_dataloader: DataLoader d'entraînement\n",
        "        val_dataloader: DataLoader de validation\n",
        "    \"\"\"\n",
        "    def get_tensor_stats(tensor: torch.Tensor) -> Optional[Tuple[float, float]]:\n",
        "        \"\"\"Calcule moyenne et écart-type d'un tensor.\"\"\"\n",
        "        try:\n",
        "            if not isinstance(tensor, torch.Tensor) or tensor.numel() == 0:\n",
        "                return None\n",
        "            return tensor.float().mean().item(), tensor.float().std().item()\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def analyze_batch(x: Dict[str, torch.Tensor], y: torch.Tensor, name: str) -> None:\n",
        "        \"\"\"Analyse détaillée d'un batch.\"\"\"\n",
        "        print(f\"\\n{name} Dataloader:\")\n",
        "\n",
        "        # Features continues (encoder et decoder)\n",
        "        for key in ['encoder_cont', 'decoder_cont']:\n",
        "            if key in x and x[key] is not None:\n",
        "                stats = get_tensor_stats(x[key])\n",
        "                if stats:\n",
        "                    mean, std = stats\n",
        "                    print(f\"  {key:12s}: Shape={x[key].shape}, Mean={mean:.3f}, Std={std:.3f}\")\n",
        "\n",
        "        # Features catégorielles (encoder et decoder)\n",
        "        for key in ['encoder_cat', 'decoder_cat']:\n",
        "            if key in x and x[key] is not None:\n",
        "                stats = get_tensor_stats(x[key])\n",
        "                if stats:\n",
        "                    mean, std = stats\n",
        "                    print(f\"  {key:12s}: Shape={x[key].shape}, Mean={mean:.3f}, Std={std:.3f}\")\n",
        "\n",
        "        # Target\n",
        "        y_tensor = y[0] if isinstance(y, (tuple, list)) else y\n",
        "        stats = get_tensor_stats(y_tensor)\n",
        "        if stats:\n",
        "            mean, std = stats\n",
        "            print(f\"  target      : Shape={y_tensor.shape}, Mean={mean:.3f}, Std={std:.3f}\")\n",
        "\n",
        "        # Informations récapitulatives\n",
        "        if 'encoder_cont' in x:\n",
        "            print(\"\\n  Récapitulatif:\")\n",
        "            print(f\"  - Batch Size      : {x['encoder_cont'].shape[0]}\")\n",
        "            print(f\"  - Sequence Length : {x['encoder_cont'].shape[1]}\")\n",
        "            print(f\"  - Nb Features     : {x['encoder_cont'].shape[2]}\")\n",
        "\n",
        "    # Vérification des dataloaders\n",
        "    for name, loader in [(\"Train\", train_dataloader), (\"Validation\", val_dataloader)]:\n",
        "        try:\n",
        "            batch = next(iter(loader))\n",
        "            x, y = batch\n",
        "            analyze_batch(x, y, name)\n",
        "        except StopIteration:\n",
        "            logger.warning(f\"Dataloader vide pour {name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erreur lors de l'analyse du {name} dataloader: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration du logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        verify_dataloaders(train_dataloader, val_dataloader)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur lors de la vérification des dataloaders: {e}\")\n",
        "\n",
        "# Configuration du logging si pas déjà fait\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Vérification des dataloaders\n",
        "verify_dataloaders(train_dataloader, val_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iGxZ-IVu7Jm",
        "outputId": "6e8cb96f-6562-4eff-ad2c-6700ab6675ee"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Dataloader:\n",
            "  encoder_cont: Shape=torch.Size([32, 180, 161]), Mean=-0.027, Std=0.941\n",
            "  decoder_cont: Shape=torch.Size([32, 30, 161]), Mean=-0.048, Std=1.048\n",
            "  encoder_cat : Shape=torch.Size([32, 180, 1]), Mean=2.938, Std=2.022\n",
            "  decoder_cat : Shape=torch.Size([32, 30, 1]), Mean=2.997, Std=2.021\n",
            "  target      : Shape=torch.Size([32, 30]), Mean=1.961, Std=23.380\n",
            "\n",
            "  Récapitulatif:\n",
            "  - Batch Size      : 32\n",
            "  - Sequence Length : 180\n",
            "  - Nb Features     : 161\n",
            "\n",
            "Validation Dataloader:\n",
            "  encoder_cont: Shape=torch.Size([1, 180, 161]), Mean=0.058, Std=0.762\n",
            "  decoder_cont: Shape=torch.Size([1, 30, 161]), Mean=0.196, Std=0.752\n",
            "  encoder_cat : Shape=torch.Size([1, 180, 1]), Mean=3.028, Std=1.998\n",
            "  decoder_cat : Shape=torch.Size([1, 30, 1]), Mean=2.833, Std=2.069\n",
            "  target      : Shape=torch.Size([1, 30]), Mean=11.710, Std=6.113\n",
            "\n",
            "  Récapitulatif:\n",
            "  - Batch Size      : 1\n",
            "  - Sequence Length : 180\n",
            "  - Nb Features     : 161\n",
            "\n",
            "Train Dataloader:\n",
            "  encoder_cont: Shape=torch.Size([32, 180, 161]), Mean=0.034, Std=0.984\n",
            "  decoder_cont: Shape=torch.Size([32, 30, 161]), Mean=0.115, Std=0.931\n",
            "  encoder_cat : Shape=torch.Size([32, 180, 1]), Mean=2.925, Std=2.029\n",
            "  decoder_cat : Shape=torch.Size([32, 30, 1]), Mean=2.977, Std=2.020\n",
            "  target      : Shape=torch.Size([32, 30]), Mean=11.209, Std=22.396\n",
            "\n",
            "  Récapitulatif:\n",
            "  - Batch Size      : 32\n",
            "  - Sequence Length : 180\n",
            "  - Nb Features     : 161\n",
            "\n",
            "Validation Dataloader:\n",
            "  encoder_cont: Shape=torch.Size([1, 180, 161]), Mean=0.058, Std=0.762\n",
            "  decoder_cont: Shape=torch.Size([1, 30, 161]), Mean=0.196, Std=0.752\n",
            "  encoder_cat : Shape=torch.Size([1, 180, 1]), Mean=3.028, Std=1.998\n",
            "  decoder_cat : Shape=torch.Size([1, 30, 1]), Mean=2.833, Std=2.069\n",
            "  target      : Shape=torch.Size([1, 30]), Mean=11.710, Std=6.113\n",
            "\n",
            "  Récapitulatif:\n",
            "  - Batch Size      : 1\n",
            "  - Sequence Length : 180\n",
            "  - Nb Features     : 161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Configuration et Entraînement du Modèle TFT\n",
        "# -----------------------------------------------\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_forecasting import TemporalFusionTransformer\n",
        "from pytorch_forecasting.metrics import QuantileLoss\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
        "import torch\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from typing import Tuple, Dict, Optional\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TFTModel(pl.LightningModule):\n",
        "    \"\"\"Modèle TFT avec Lightning.\"\"\"\n",
        "\n",
        "    def __init__(self, tft_model: TemporalFusionTransformer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tft_model: Instance de TemporalFusionTransformer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = tft_model\n",
        "        self.save_hyperparameters(ignore=['tft_model'])\n",
        "        self.loss = QuantileLoss()\n",
        "\n",
        "    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Forward pass avec gestion des types de tenseurs.\"\"\"\n",
        "        # Conversion des tenseurs\n",
        "        x = {\n",
        "            k: v.long() if k.endswith('_cat') else v.to(self.device)\n",
        "            for k, v in x.items()\n",
        "            if torch.is_tensor(v)\n",
        "        }\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch: Tuple[Dict, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "        \"\"\"Étape d'entraînement.\"\"\"\n",
        "        x, y = batch\n",
        "        y = y[0] if isinstance(y, tuple) else y\n",
        "        loss = self.loss(self(x).prediction, y)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: Tuple[Dict, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "        \"\"\"Étape de validation.\"\"\"\n",
        "        x, y = batch\n",
        "        y = y[0] if isinstance(y, tuple) else y\n",
        "        loss = self.loss(self(x).prediction, y)\n",
        "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self) -> Dict:\n",
        "        \"\"\"Configuration de l'optimiseur et du scheduler.\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.model.hparams.learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode=\"min\",\n",
        "            factor=0.1,\n",
        "            patience=self.model.hparams.reduce_on_plateau_patience,\n",
        "            verbose=True,\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\", \"frequency\": 1},\n",
        "        }\n",
        "\n",
        "def setup_trainer(config: Dict) -> Tuple[pl.Trainer, pl.loggers.WandbLogger]:\n",
        "    \"\"\"\n",
        "    Configure le trainer et le logger.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration du modèle\n",
        "\n",
        "    Returns:\n",
        "        trainer: Instance de Trainer configurée\n",
        "        logger: Instance de WandbLogger\n",
        "    \"\"\"\n",
        "    model_config = config['model']\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            patience=model_config['early_stopping']['patience'],\n",
        "            mode=\"min\"\n",
        "        ),\n",
        "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
        "        ModelCheckpoint(\n",
        "            monitor=\"val_loss\",\n",
        "            dirpath=\"checkpoints\",\n",
        "            filename=\"tft-{epoch:02d}-{val_loss:.2f}\",\n",
        "            save_top_k=3,\n",
        "            mode=\"min\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Logger\n",
        "    wandb_logger = pl.loggers.WandbLogger(\n",
        "        project=\"stock_predictions_TFT\",\n",
        "        name=f\"tft_{config['data']['symbol']}_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
        "        log_model=True\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=model_config['training']['max_epochs']['value'],\n",
        "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "        devices=1,\n",
        "        gradient_clip_val=model_config['training']['gradient_clip_val'],\n",
        "        callbacks=callbacks,\n",
        "        logger=wandb_logger,\n",
        "        precision=32\n",
        "    )\n",
        "\n",
        "    return trainer, wandb_logger\n",
        "\n",
        "def train_model(\n",
        "    training_dataset: TimeSeriesDataSet,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    val_dataloader: torch.utils.data.DataLoader,\n",
        "    config: Dict\n",
        ") -> Tuple[TFTModel, pl.Trainer]:\n",
        "    \"\"\"\n",
        "    Entraîne le modèle TFT.\n",
        "\n",
        "    Args:\n",
        "        training_dataset: Dataset d'entraînement\n",
        "        train_dataloader: DataLoader d'entraînement\n",
        "        val_dataloader: DataLoader de validation\n",
        "        config: Configuration du modèle\n",
        "\n",
        "    Returns:\n",
        "        model: Modèle entraîné\n",
        "        trainer: Trainer utilisé\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configuration du trainer et du logger\n",
        "        trainer, logger = setup_trainer(config)\n",
        "\n",
        "        # Création du modèle TFT\n",
        "        architecture = config['model']['architecture']\n",
        "        tft = TemporalFusionTransformer.from_dataset(\n",
        "            training_dataset,\n",
        "            learning_rate=config['model']['training']['learning_rate']['value'],\n",
        "            hidden_size=architecture['hidden_size']['value'],\n",
        "            attention_head_size=architecture['attention_head_size']['value'],\n",
        "            dropout=architecture['dropout']['value'],\n",
        "            hidden_continuous_size=architecture['hidden_continuous_size']['value'],\n",
        "            loss=QuantileLoss(),\n",
        "            log_interval=architecture['log_interval']\n",
        "        )\n",
        "\n",
        "        # Création du modèle Lightning\n",
        "        model = TFTModel(tft)\n",
        "\n",
        "        # Log des hyperparamètres\n",
        "        logger.log_hyperparams(architecture)\n",
        "\n",
        "        # Entraînement\n",
        "        trainer.fit(model, train_dataloader, val_dataloader)\n",
        "\n",
        "        return model, trainer\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur pendant l'entraînement: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        # Initialisation de la configuration\n",
        "        config = init_wandb_config(stock_symbol='BTC-USD', mode='disabled')\n",
        "\n",
        "        # Entraînement\n",
        "        model, trainer = train_model(training_dataset, train_dataloader, val_dataloader, config)\n",
        "        logger.info(\"Entraînement terminé avec succès\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur: {str(e)}\")\n",
        "\n",
        "\n",
        "model, trainer, test_data = run_complete_pipeline(symbol='BTC-USD')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "24526e4096e6417dbbf8e38f7831011f",
            "8300aa916c7a48fbb9b64428df6eded0",
            "c73a1b0039c540919bf397fa0014dda3",
            "6071e18bbe934a189b1cdf8e80f854ec",
            "a9e09000d1af48428cc189f537f53d62",
            "f3b96a9b83454bf08800bbfac6d8ce59",
            "d31a0f5af3f9420683e536366447ffef",
            "1b0807a06d534c79932c43f788d45a6f",
            "2434980310124907a7141e9a2d7c5616",
            "75a89e514aff44308a0cbc5cf6beee01",
            "80bb307d55f948d888bc23015cc47bc3",
            "2200c7532a054ac19d6b8b4e1d799eab",
            "d19815c88fb0469eb6a871c9b37ad3e8",
            "fbe0ceaaaead414b87819c423333ad1b",
            "edcb72d0812043d08503b2bb2d384260",
            "8d009ddabff9490fb6ca61e94bd6c857",
            "f7d22425dbf04334810cae6923e033ac",
            "5437fad04e8f432cbad851dd4bbe6f32",
            "92bad7796ec04cea87e6907fad4be3e4",
            "b228479e6977438c8f25bc3a631910eb",
            "660be3b0a0ef44ac8c7b2204db8cf49b",
            "52e5fb9a274c49a687ce1ec5d7e7af96"
          ]
        },
        "id": "-5zRzQ7bKqsr",
        "outputId": "ab603053-94d6-4f57-ad65-a3d5dcb7523e"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Erreur: name 'training_dataset' is not defined\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialisation de la configuration WandB...\n",
            "Configuration WandB initialisée avec succès.\n",
            "Initialisation de la configuration WandB...\n",
            "Configuration WandB initialisée avec succès.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-249-1feb93e3a69f>:103: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n",
            "<ipython-input-249-1feb93e3a69f>:169: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill')\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries.py:1207: UserWarning: If predicting, no randomization should be possible - setting stop_randomization=True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Dataloader:\n",
            "  encoder_cont: Shape=torch.Size([32, 180, 161]), Mean=0.009, Std=0.963\n",
            "  decoder_cont: Shape=torch.Size([32, 30, 161]), Mean=0.027, Std=0.970\n",
            "  encoder_cat : Shape=torch.Size([32, 180, 1]), Mean=2.977, Std=2.011\n",
            "  decoder_cat : Shape=torch.Size([32, 30, 1]), Mean=2.983, Std=1.997\n",
            "  target      : Shape=torch.Size([32, 30]), Mean=8.981, Std=23.582\n",
            "\n",
            "  Récapitulatif:\n",
            "  - Batch Size      : 32\n",
            "  - Sequence Length : 180\n",
            "  - Nb Features     : 161\n",
            "\n",
            "Validation Dataloader:\n",
            "  encoder_cont: Shape=torch.Size([1, 180, 161]), Mean=0.058, Std=0.762\n",
            "  decoder_cont: Shape=torch.Size([1, 30, 161]), Mean=0.196, Std=0.752\n",
            "  encoder_cat : Shape=torch.Size([1, 180, 1]), Mean=3.028, Std=1.998\n",
            "  decoder_cat : Shape=torch.Size([1, 30, 1]), Mean=2.833, Std=2.069\n",
            "  target      : Shape=torch.Size([1, 30]), Mean=11.710, Std=6.113\n",
            "\n",
            "  Récapitulatif:\n",
            "  - Batch Size      : 1\n",
            "  - Sequence Length : 180\n",
            "  - Nb Features     : 161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
            "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
            "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type                      | Params | Mode \n",
            "------------------------------------------------------------\n",
            "0 | model | TemporalFusionTransformer | 1.6 M  | train\n",
            "1 | loss  | QuantileLoss              | 0      | train\n",
            "------------------------------------------------------------\n",
            "1.6 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.6 M     Total params\n",
            "6.591     Total estimated model params size (MB)\n",
            "2558      Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24526e4096e6417dbbf8e38f7831011f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2200c7532a054ac19d6b8b4e1d799eab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
            "ERROR:__main__:Erreur dans le pipeline: 'WandbLogger' object has no attribute 'error'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'WandbLogger' object has no attribute 'error'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \"\"\"\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mbackward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"backward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-264-62721f239393>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(training_dataset, train_dataloader, val_dataloader, config)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Entraînement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-264-62721f239393>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_complete_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BTC-USD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-262-aafecda80680>\u001b[0m in \u001b[0;36mrun_complete_pipeline\u001b[0;34m(symbol, mode)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# 7. Entraînement du modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         model, trainer = train_model(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-264-62721f239393>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(training_dataset, train_dataloader, val_dataloader, config)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Erreur pendant l'entraînement: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'WandbLogger' object has no attribute 'error'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# Bloc : Evaluation du modèle et Prédictions\n",
        "# -----------------------------------------------\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "import wandb\n",
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from pytorch_forecasting import TimeSeriesDataSet\n",
        "\n",
        "def evaluate_and_predict(model, training_dataset, test_data, config):\n",
        "    \"\"\"\n",
        "    Évalue le modèle et génère des prédictions avec logging WandB.\n",
        "    \"\"\"\n",
        "    # 0. Configuration initiale\n",
        "    eval_config = config['evaluation']\n",
        "    device = next(model.parameters()).device\n",
        "    print(f\"Évaluation sur device: {device}\")\n",
        "\n",
        "    # Accéder au normalizer\n",
        "    target_normalizer = training_dataset.target_normalizer\n",
        "\n",
        "    def validate_predictions(predictions, name=\"predictions\"):\n",
        "        \"\"\"Valide la qualité des prédictions.\"\"\"\n",
        "        issues = []\n",
        "        if np.isnan(predictions).any():\n",
        "            issues.append(\"NaN détectés\")\n",
        "        if np.isinf(predictions).any():\n",
        "            issues.append(\"Inf détectés\")\n",
        "        if len(issues) > 0:\n",
        "            print(f\"Warning - {name}: {', '.join(issues)}\")\n",
        "        return len(issues) == 0\n",
        "\n",
        "    def calculate_rolling_metrics(actual, predicted, window=30):\n",
        "        \"\"\"Calcule les métriques sur une fenêtre glissante.\"\"\"\n",
        "        rolling_metrics = {\n",
        "            'mae': [],\n",
        "            'mape': [],\n",
        "            'r2': []\n",
        "        }\n",
        "        for i in range(len(actual) - window + 1):\n",
        "            slice_actual = actual[i:i+window]\n",
        "            slice_pred = predicted[i:i+window]\n",
        "            rolling_metrics['mae'].append(\n",
        "                mean_absolute_error(slice_actual, slice_pred))\n",
        "            rolling_metrics['mape'].append(\n",
        "                np.mean(np.abs((slice_actual - slice_pred) / slice_actual)) * 100)\n",
        "            rolling_metrics['r2'].append(\n",
        "                r2_score(slice_actual, slice_pred))\n",
        "        return rolling_metrics\n",
        "\n",
        "    # 1. Préparation des données\n",
        "    test_data = test_data.reset_index()\n",
        "    test_data['time_idx'] = range(len(test_data))\n",
        "    test_data['series_id'] = 'ACCOR'\n",
        "    test_data['Day_of_Week'] = test_data['Day_of_Week'].astype(str)\n",
        "\n",
        "    # 2. Création du dataset et dataloader\n",
        "    predict_dataset = TimeSeriesDataSet.from_dataset(\n",
        "        training_dataset,\n",
        "        test_data,\n",
        "        stop_randomization=True,\n",
        "        predict=True\n",
        "    )\n",
        "\n",
        "    predict_dataloader = predict_dataset.to_dataloader(\n",
        "        train=False,\n",
        "        batch_size=eval_config['prediction']['batch_size'],\n",
        "        num_workers=eval_config['prediction']['num_workers'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # 3. Génération des prédictions\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_dates = []\n",
        "    quantiles_predictions = []  # Pour les intervalles de confiance\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (batch, _) in enumerate(predict_dataloader):\n",
        "            # Déplacer les données sur le bon device\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
        "                    for k, v in batch.items()}\n",
        "\n",
        "            outputs = model.model(batch)  # Accès correct à TFT model\n",
        "\n",
        "            # Vérifier la forme de outputs.prediction\n",
        "            # Supposons que outputs.prediction a la forme [batch, prediction_length, num_quantiles]\n",
        "            pred_mean = outputs.prediction.mean(dim=-1)\n",
        "            all_predictions.append(pred_mean)\n",
        "\n",
        "            # Extraire les quantiles spécifiques, par exemple 0.1 et 0.9\n",
        "            pred_lower = outputs.prediction[..., 0]  # Premier quantile\n",
        "            pred_upper = outputs.prediction[..., -1]  # Dernier quantile\n",
        "            quantiles_predictions.extend(zip(pred_lower.flatten(), pred_upper.flatten()))\n",
        "\n",
        "            # Extraire les dates\n",
        "            dates = batch.get('decoder_time_idx', None)\n",
        "            if dates is not None:\n",
        "                dates = dates.cpu().numpy()\n",
        "                all_dates.extend(dates.flatten())\n",
        "            else:\n",
        "                # Si 'decoder_time_idx' n'est pas disponible, utilisez 'time_idx'\n",
        "                dates = batch.get('time_idx', None)\n",
        "                if dates is not None:\n",
        "                    dates = dates.cpu().numpy()\n",
        "                    all_dates.extend(dates.flatten())\n",
        "\n",
        "            # Log de la progression\n",
        "            wandb.log({\n",
        "                \"prediction_progress\": batch_idx / len(predict_dataloader)\n",
        "            })\n",
        "\n",
        "    # Concatenation des prédictions\n",
        "    all_predictions = torch.cat(all_predictions).to(device)\n",
        "    # Inverse transformation des prédictions\n",
        "    all_predictions = target_normalizer.inverse_transform({\"prediction\": all_predictions})[\"prediction\"].cpu().numpy()\n",
        "\n",
        "    # 4. Création du DataFrame des prédictions\n",
        "    predictions = pd.DataFrame(\n",
        "        all_predictions,\n",
        "        index=all_dates,\n",
        "        columns=['Predicted_Close']\n",
        "    )\n",
        "\n",
        "    # Ajout des intervalles de confiance si disponibles\n",
        "    if len(quantiles_predictions) > 0:\n",
        "        quantiles = torch.tensor(quantiles_predictions, dtype=torch.float32).to(device)\n",
        "        quantiles = target_normalizer.inverse_transform({\"prediction\": quantiles})[\"prediction\"].cpu().numpy()\n",
        "        predictions['Lower_bound'] = quantiles[:, 0]\n",
        "        predictions['Upper_bound'] = quantiles[:, 1]\n",
        "\n",
        "    predictions = predictions.loc[~predictions.index.duplicated(keep='first')]\n",
        "\n",
        "    # Inverse transformation des valeurs réelles\n",
        "    actual_prices_tensor = torch.from_numpy(test_data.loc[predictions.index, 'Close'].values.reshape(-1, 1)).to(device)\n",
        "    actual_prices = target_normalizer.inverse_transform({\"prediction\": actual_prices_tensor})[\"prediction\"].cpu().numpy().flatten()\n",
        "\n",
        "    # Validation des prédictions\n",
        "    if not validate_predictions(predictions['Predicted_Close'].values):\n",
        "        print(\"Attention : Les prédictions contiennent des valeurs invalides.\")\n",
        "\n",
        "    # 5. Calcul des métriques\n",
        "    metrics = {\n",
        "        'mae': mean_absolute_error(actual_prices, predictions['Predicted_Close']),\n",
        "        'mse': mean_squared_error(actual_prices, predictions['Predicted_Close']),\n",
        "        'rmse': np.sqrt(mean_squared_error(actual_prices, predictions['Predicted_Close'])),\n",
        "        'r2': r2_score(actual_prices, predictions['Predicted_Close']),\n",
        "        'mape': np.mean(np.abs((actual_prices - predictions['Predicted_Close']) / actual_prices)) * 100,\n",
        "    }\n",
        "\n",
        "    # Calcul du MASE\n",
        "    naive_forecast = actual_prices[:-1]\n",
        "    naive_errors = np.abs(np.diff(actual_prices))\n",
        "    metrics['mase'] = metrics['mae'] / np.mean(naive_errors)\n",
        "\n",
        "    # Calcul de la précision directionnelle\n",
        "    direction_actual = np.sign(np.diff(actual_prices))\n",
        "    direction_pred = np.sign(np.diff(predictions['Predicted_Close'].values))\n",
        "    directional_accuracy = np.mean(direction_actual == direction_pred) * 100\n",
        "    metrics['directional_accuracy'] = directional_accuracy\n",
        "\n",
        "    # 6. Calcul des métriques glissantes\n",
        "    rolling_metrics = calculate_rolling_metrics(\n",
        "        actual_prices,\n",
        "        predictions['Predicted_Close'].values\n",
        "    )\n",
        "\n",
        "    # 7. Visualisations\n",
        "    fig = plt.figure(figsize=(15, 12))\n",
        "    gs = plt.GridSpec(2, 1, height_ratios=[3, 1])\n",
        "\n",
        "    # Plot principal\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    ax1.plot(test_data.loc[predictions.index, 'Close'].index, actual_prices, label='Prix réels', color='blue')\n",
        "    ax1.plot(predictions.index, predictions['Predicted_Close'], label='Prédictions', color='orange')\n",
        "\n",
        "    if 'Lower_bound' in predictions.columns:\n",
        "        ax1.fill_between(predictions.index,\n",
        "                         predictions['Lower_bound'],\n",
        "                         predictions['Upper_bound'],\n",
        "                         color='orange', alpha=0.2,\n",
        "                         label='Intervalle de confiance')\n",
        "\n",
        "    # Affichage des métriques\n",
        "    metrics_text = (\n",
        "        f\"MAE: {metrics['mae']:.2f}\\n\"\n",
        "        f\"RMSE: {metrics['rmse']:.2f}\\n\"\n",
        "        f\"R²: {metrics['r2']:.4f}\\n\"\n",
        "        f\"MAPE: {metrics['mape']:.2f}%\\n\"\n",
        "        f\"MASE: {metrics['mase']:.4f}\\n\"\n",
        "        f\"Dir. Acc: {metrics['directional_accuracy']:.1f}%\"\n",
        "    )\n",
        "\n",
        "    ax1.text(0.02, 0.98, metrics_text,\n",
        "            transform=ax1.transAxes,\n",
        "            bbox=dict(facecolor='white', alpha=0.8),\n",
        "            verticalalignment='top',\n",
        "            fontsize=10)\n",
        "\n",
        "    ax1.set_title('Prédictions vs Prix réels')\n",
        "    ax1.set_ylabel('Prix de clôture (EUR)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot des erreurs\n",
        "    ax2 = fig.add_subplot(gs[1])\n",
        "    errors = actual_prices - predictions['Predicted_Close'].values\n",
        "    ax2.plot(predictions.index, errors, color='red', label='Erreur')\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "    ax2.set_xlabel('Date')\n",
        "    ax2.set_ylabel('Erreur')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Loguer le graphique avant de l'afficher ou le fermer\n",
        "    wandb.log({\n",
        "        \"predictions_plot\": wandb.Image(fig),\n",
        "        \"metrics\": metrics,\n",
        "        \"rolling_metrics\": {\n",
        "            f\"rolling_{k}\": v for k, v in rolling_metrics.items()\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # Afficher le graphique dans le notebook\n",
        "    plt.show()\n",
        "\n",
        "    # 8. Sauvegarde des prédictions\n",
        "    predictions_path = os.path.join(wandb.run.dir, \"predictions.csv\")\n",
        "    predictions.to_csv(predictions_path)\n",
        "    wandb.save(predictions_path)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    # 9. Affichage des résultats\n",
        "    print(\"\\nMétriques d'évaluation détaillées:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name.upper()}: {value:.4f}\")\n",
        "\n",
        "    # 10. Analyse statistique détaillée\n",
        "    error_stats = pd.DataFrame({\n",
        "        'Prix_reels': actual_prices,\n",
        "        'Prix_predits': predictions['Predicted_Close'].values,\n",
        "        'Erreur_absolue': abs(errors),\n",
        "        'Erreur_relative': (abs(errors) / actual_prices) * 100\n",
        "    })\n",
        "\n",
        "    wandb.log({\n",
        "        \"error_statistics\": wandb.Table(dataframe=error_stats.describe().round(2))\n",
        "    })\n",
        "\n",
        "    return predictions, metrics, error_stats\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Bloc : Utilisation de la Fonction d'Évaluation\n",
        "# -----------------------------------------------\n",
        "try:\n",
        "    # Remplacez 'run.config' par 'config'\n",
        "    predictions, metrics, error_stats = evaluate_and_predict(\n",
        "        model,\n",
        "        training_dataset,\n",
        "        test_data,\n",
        "        config  # Utilisez 'config' au lieu de 'run.config'\n",
        "    )\n",
        "    print(\"\\nÉvaluation terminée avec succès!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors de l'évaluation: {e}\")\n",
        "    # Optionnel : Loguer l'erreur dans WandB\n",
        "    wandb.log({\"evaluation_error\": str(e)})\n",
        "    # Afficher la trace complète de l'erreur pour le débogage\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "xnkkI7D-Lj59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cloture de la session WandB\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "LNYlTVKEVmuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ARSDysrVnKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}